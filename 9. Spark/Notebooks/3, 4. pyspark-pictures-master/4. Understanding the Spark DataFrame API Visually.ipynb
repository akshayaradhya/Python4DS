{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* [RDD API](http://nbviewer.ipython.org/github/jkthompson/pyspark-pictures/blob/master/pyspark-pictures.ipynb)\n",
    "* [GitHub](https://github.com/jkthompson/pyspark-pictures)\n",
    "* [related blog post](http://data-frack.blogspot.com/2015/01/visual-mnemonics-for-pyspark-api.html)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img align=left src=\"files/images/pyspark-pictures-dataframes-page2.svg\" width=500 height=500 />"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "> ###  Click on a picture to view pyspark docs\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# versions\n",
    "import IPython\n",
    "print(\"pyspark version:\" + str(sc.version))\n",
    "print(\"Ipython version:\" + str(IPython.__version__))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<pyspark.context.SparkContext at 0x7fce82310dd0>"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "\n",
    "spark = SparkSession \\\n",
    "    .builder \\\n",
    "    .appName(\"Python Spark SQL basic example\") \\\n",
    "    .config(\"spark.some.config.option\", \"some-value\") \\\n",
    "    .getOrCreate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----+-----+---+\n",
      "| from|   to|amt|\n",
      "+-----+-----+---+\n",
      "|Alice|  Bob|0.1|\n",
      "|  Bob|Carol|0.2|\n",
      "|Carol| Dave|0.3|\n",
      "+-----+-----+---+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "x = spark.createDataFrame([(\"Alice\",\"Bob\",0.1),(\"Bob\",\"Carol\",0.2),(\"Carol\",\"Dave\",0.3)], ['from','to','amt'])\n",
    "x.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a href=\"http://spark.apache.org/docs/latest/api/python/pyspark.sql.html#pyspark.sql.DataFrame.agg\">\n",
    "<img align=left src=\"files/images/pyspark-pictures-dataframes-page3.svg\" width=500 height=500 />\n",
    "</a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "x.agg?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------------------+\n",
      "|           avg(amt)|\n",
      "+-------------------+\n",
      "|0.20000000000000004|\n",
      "+-------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# agg\n",
    "y = x.agg({\"amt\":\"avg\"})\n",
    "\n",
    "y.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a href=\"http://spark.apache.org/docs/latest/api/python/pyspark.sql.html#pyspark.sql.DataFrame.alias\">\n",
    "<img align=left src=\"files/images/pyspark-pictures-dataframes-page4.svg\" width=750 height=750 />\n",
    "</a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "x.alias?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----+-----+---+\n",
      "| from|   to|amt|\n",
      "+-----+-----+---+\n",
      "|Alice|  Bob|0.1|\n",
      "|  Bob|Carol|0.2|\n",
      "|Carol| Dave|0.3|\n",
      "+-----+-----+---+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# alias\n",
    "from pyspark.sql.functions import col\n",
    "\n",
    "y = x.alias('transactions')\n",
    "\n",
    "y.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----+\n",
      "|   to|\n",
      "+-----+\n",
      "|  Bob|\n",
      "|Carol|\n",
      "| Dave|\n",
      "+-----+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "y.select(\"to\").show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----+\n",
      "|   to|\n",
      "+-----+\n",
      "|  Bob|\n",
      "|Carol|\n",
      "| Dave|\n",
      "+-----+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "y.select(col(\"transactions.to\")).show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a href=\"http://spark.apache.org/docs/latest/api/python/pyspark.sql.html#pyspark.sql.DataFrame.cache\">\n",
    "<img align=left src=\"files/images/pyspark-pictures-dataframes-page5.svg\" width=750 height=750 />\n",
    "</a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# cache\n",
    "x.cache()\n",
    "\n",
    "print(x.count()) # first action materializes x in memory\n",
    "print(x.count()) # later actions avoid IO overhead"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a href=\"http://spark.apache.org/docs/latest/api/python/pyspark.sql.html#pyspark.sql.DataFrame.coalesce\">\n",
    "<img align=left src=\"files/images/pyspark-pictures-dataframes-page6.svg\" width=750 height=750 />\n",
    "</a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "x.rdd.take(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2\n",
      "1\n"
     ]
    }
   ],
   "source": [
    "# coalesce\n",
    "x_rdd = sc.parallelize([(\"Alice\",\"Bob\",0.1),\n",
    "                        (\"Bob\",\"Carol\",0.2),\n",
    "                        (\"Carol\",\"Dave\",0.3)], 2)\n",
    "\n",
    "x = sqlContext.createDataFrame(x_rdd, ['from','to','amt'])\n",
    "\n",
    "y = x.coalesce(numPartitions=1)\n",
    "\n",
    "print(x.rdd.getNumPartitions())\n",
    "print(y.rdd.getNumPartitions())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a href=\"http://spark.apache.org/docs/latest/api/python/pyspark.sql.html#pyspark.sql.DataFrame.collect\">\n",
    "<img align=left src=\"files/images/pyspark-pictures-dataframes-page7.svg\" width=750 height=750 />\n",
    "</a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# collect\n",
    "y = x.collect() # creates list of rows on driver\n",
    "\n",
    "x.show()\n",
    "\n",
    "print(y)\n",
    "print type(y)\n",
    "print type(x.rdd)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a href=\"http://spark.apache.org/docs/latest/api/python/pyspark.sql.html#pyspark.sql.DataFrame.columns\">\n",
    "<img align=left src=\"files/images/pyspark-pictures-dataframes-page8.svg\" width=500 height=500 />\n",
    "</a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# columns\n",
    "y = x.columns \n",
    "#creates list of column names on driver\n",
    "\n",
    "print(y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a href=\"http://spark.apache.org/docs/latest/api/python/pyspark.sql.html#pyspark.sql.DataFrame.corr\">\n",
    "<img align=left src=\"files/images/pyspark-pictures-dataframes-page9.svg\" width=500 height=500 />\n",
    "</a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.866025403784\n",
      "<type 'float'>\n"
     ]
    }
   ],
   "source": [
    "# corr\n",
    "x = sqlContext.createDataFrame([(\"Alice\",\"Bob\",0.1,0.001),\n",
    "                                (\"Bob\",\"Carol\",0.2,0.02),\n",
    "                                (\"Carol\",\"Dave\",0.3,0.02)], schema=['from','to','amt','fee'])\n",
    "\n",
    "y = x.corr(col1=\"amt\",col2=\"fee\")\n",
    "\n",
    "print(y)\n",
    "print(type(y))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a href=\"http://spark.apache.org/docs/latest/api/python/pyspark.sql.html#pyspark.sql.DataFrame.count\">\n",
    "<img align=left src=\"files/images/pyspark-pictures-dataframes-page10.svg\" width=500 height=500 />\n",
    "</a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# count\n",
    "\n",
    "print(x.count())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a href=\"http://spark.apache.org/docs/latest/api/python/pyspark.sql.html#pyspark.sql.DataFrame.cov\">\n",
    "<img align=left src=\"files/images/pyspark-pictures-dataframes-page11.svg\" width=500 height=500 />\n",
    "</a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# cov\n",
    "x = sqlContext.createDataFrame([(\"Alice\",\"Bob\",0.1,0.001),(\"Bob\",\"Carol\",0.2,0.02),(\"Carol\",\"Dave\",0.3,0.02)], ['from','to','amt','fee'])\n",
    "y = x.cov(col1=\"amt\",col2=\"fee\")\n",
    "x.show()\n",
    "print(y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a href=\"http://spark.apache.org/docs/latest/api/python/pyspark.sql.html#pyspark.sql.DataFrame.crosstab\">\n",
    "<img align=left src=\"files/images/pyspark-pictures-dataframes-page12.svg\" width=500 height=500 />\n",
    "</a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----+-----+---+\n",
      "| from|   to|amt|\n",
      "+-----+-----+---+\n",
      "|Alice|  Bob|0.1|\n",
      "|  Bob|Carol|0.2|\n",
      "|Carol| Dave|0.3|\n",
      "+-----+-----+---+\n",
      "\n",
      "+-------+---+-----+----+\n",
      "|from_to|Bob|Carol|Dave|\n",
      "+-------+---+-----+----+\n",
      "|    Bob|  0|    1|   0|\n",
      "|  Alice|  1|    0|   0|\n",
      "|  Carol|  0|    0|   1|\n",
      "+-------+---+-----+----+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# crosstab\n",
    "x = sqlContext.createDataFrame([(\"Alice\",\"Bob\",0.1),\n",
    "                                (\"Bob\",\"Carol\",0.2),\n",
    "                                (\"Carol\",\"Dave\",0.3)], ['from','to','amt'])\n",
    "y = x.crosstab(col1='from', col2='to')\n",
    "x.show()\n",
    "y.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "> ## TASK\n",
    "\n",
    "1.\n",
    "- Create a pandas dataframe with 2 columns called 'Actual', 'Predicted'.\n",
    "- Fill it with True, False values at random (50 values)\n",
    "- Convert it to a Spark Dataframe\n",
    "- Use crosstab to find the confusion matrix\n",
    "\n",
    "2.\n",
    "Then, attempt the same, but this time, use RowRDDs to construct the DataFrame before using CrossTab\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Actual</th>\n",
       "      <th>Predicted</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>T</td>\n",
       "      <td>T</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>F</td>\n",
       "      <td>T</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>T</td>\n",
       "      <td>F</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>T</td>\n",
       "      <td>T</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>F</td>\n",
       "      <td>F</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  Actual Predicted\n",
       "0      T         T\n",
       "1      F         T\n",
       "2      T         F\n",
       "3      T         T\n",
       "4      F         F"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "pdf_x = pd.DataFrame(zip(pd.Series(list('TF' * 100)).sample(50), pd.Series(list('TF' * 100)).sample(50)),\n",
    "                     columns=['Actual', 'Predicted'])\n",
    "pdf_x.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "sdf_x = spark.createDataFrame(pdf_x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------------+---+---+\n",
      "|Actual_Predicted|  F|  T|\n",
      "+----------------+---+---+\n",
      "|               F|  9| 13|\n",
      "|               T| 15| 13|\n",
      "+----------------+---+---+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "sdf_x.crosstab(col1='Actual', col2='Predicted').show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Using RowRDD"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from pyspark.sql import Row"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "rdd_x = sc.parallelize(zip(pd.Series(list('TF' * 100)).sample(50).tolist(),\n",
    "                           pd.Series(list('TF' * 100)).sample(50).tolist()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('F', 'F'), ('T', 'F'), ('F', 'F')]"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rdd_x.take(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "row_rdd_x = rdd_x.map(lambda x: Row(Actual = x[0], Predicted = x[1]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Row(Actual='F', Predicted='F'), Row(Actual='T', Predicted='F')]"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "row_rdd_x.take(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------------+---+---+\n",
      "|Actual_Predicted|  F|  T|\n",
      "+----------------+---+---+\n",
      "|               F| 16|  9|\n",
      "|               T| 11| 14|\n",
      "+----------------+---+---+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "(spark\n",
    " .createDataFrame(row_rdd_x)\n",
    " .crosstab(col1='Actual', col2='Predicted')\n",
    " .show())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a href=\"http://spark.apache.org/docs/latest/api/python/pyspark.sql.html#pyspark.sql.DataFrame.cube\">\n",
    "<img align=left src=\"files/images/pyspark-pictures-dataframes-page13.svg\" width=500 height=500 />\n",
    "</a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "x.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# cube\n",
    "y = x.cube('from','to')\n",
    "\n",
    "print(y)\n",
    "# y is a grouped data object, aggregations will be applied to all numerical columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "y.sum().show() \n",
    "y.max().show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a href=\"http://spark.apache.org/docs/latest/api/python/pyspark.sql.html#pyspark.sql.DataFrame.describe\">\n",
    "<img align=left src=\"files/images/pyspark-pictures-dataframes-page14.svg\" width=500 height=500 />\n",
    "</a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# describe\n",
    "x.describe().show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a href=\"http://spark.apache.org/docs/latest/api/python/pyspark.sql.html#pyspark.sql.DataFrame.distinct\">\n",
    "<img align=left src=\"files/images/pyspark-pictures-dataframes-page15.svg\" width=500 height=500 />\n",
    "</a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# distinct\n",
    "y = x.distinct()\n",
    "x.show()\n",
    "y.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a href=\"http://spark.apache.org/docs/latest/api/python/pyspark.sql.html#pyspark.sql.DataFrame.drop\">\n",
    "<img align=left src=\"files/images/pyspark-pictures-dataframes-page16.svg\" width=500 height=500 />\n",
    "</a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----+-----+---+\n",
      "| from|   to|amt|\n",
      "+-----+-----+---+\n",
      "|Alice|  Bob|0.1|\n",
      "|  Bob|Carol|0.2|\n",
      "|Carol| Dave|0.3|\n",
      "+-----+-----+---+\n",
      "\n",
      "+-----+-----+\n",
      "| from|   to|\n",
      "+-----+-----+\n",
      "|Alice|  Bob|\n",
      "|  Bob|Carol|\n",
      "|Carol| Dave|\n",
      "+-----+-----+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# drop\n",
    "# x = sqlContext.createDataFrame([(\"Alice\",\"Bob\",0.1),(\"Bob\",\"Carol\",0.2),(\"Carol\",\"Dave\",0.3)], ['from','to','amt'])\n",
    "y = x.drop('amt')\n",
    "x.show()\n",
    "y.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a href=\"http://spark.apache.org/docs/latest/api/python/pyspark.sql.html#pyspark.sql.DataFrame.dropDuplicates\">\n",
    "<img align=left src=\"files/images/pyspark-pictures-dataframes-page17.svg\" width=500 height=500 />\n",
    "</a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# dropDuplicates / drop_duplicates\n",
    "x = sqlContext.createDataFrame([(\"Alice\",\"Bob\",0.1),(\"Bob\",\"Carol\",0.2),(\"Bob\",\"Carol\",0.3),(\"Bob\",\"Carol\",0.2)], ['from','to','amt'])\n",
    "y = x.dropDuplicates(subset=['from','to'])\n",
    "x.show()\n",
    "y.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a href=\"http://spark.apache.org/docs/latest/api/python/pyspark.sql.html#pyspark.sql.DataFrame.dropna\">\n",
    "<img align=left src=\"files/images/pyspark-pictures-dataframes-page18.svg\" width=500 height=500 />\n",
    "<"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# dropna\n",
    "x = sqlContext.createDataFrame([(None,\"Bob\",0.1),(\"Bob\",\"Carol\",None),(\"Carol\",None,0.3),(\"Bob\",\"Carol\",0.2)], ['from','to','amt'])\n",
    "y = x.dropna(how='any',subset=['from','to'])\n",
    "x.show()\n",
    "y.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a href=\"http://spark.apache.org/docs/latest/api/python/pyspark.sql.html#pyspark.sql.DataFrame.dtypes\">\n",
    "<img align=left src=\"files/images/pyspark-pictures-dataframes-page19.svg\" width=500 height=500 />\n",
    "</a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# dtypes\n",
    "x = sqlContext.createDataFrame([('Alice',\"Bob\",0.1),(\"Bob\",\"Carol\",0.2),(\"Carol\",\"Dave\",0.3)], ['from','to','amt'])\n",
    "y = x.dtypes\n",
    "x.show()\n",
    "print(y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a href=\"http://spark.apache.org/docs/latest/api/python/pyspark.sql.html#pyspark.sql.DataFrame.explain\">\n",
    "<img align=left src=\"files/images/pyspark-pictures-dataframes-page20.svg\" width=500 height=500 />\n",
    "</a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "x.explain?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# explain\n",
    "x = sqlContext.createDataFrame([('Alice',\"Bob\",0.1),(\"Bob\",\"Carol\",0.2),(\"Carol\",\"Dave\",0.3)], ['from','to','amt'])\n",
    "x.show()\n",
    "x.agg({\"amt\":\"avg\"}).explain(extended = True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a href=\"http://spark.apache.org/docs/latest/api/python/pyspark.sql.html#pyspark.sql.DataFrame.fillna\">\n",
    "<img align=left src=\"files/images/pyspark-pictures-dataframes-page21.svg\" width=500 height=500 />\n",
    "</a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# fillna\n",
    "x = sqlContext.createDataFrame([(None,\"Bob\",0.1),(\"Bob\",\"Carol\",None),(\"Carol\",None,0.3)], ['from','to','amt'])\n",
    "y = x.fillna(value='Missing', subset=['from'])\n",
    "x.show()\n",
    "y.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a href=\"http://spark.apache.org/docs/latest/api/python/pyspark.sql.html#pyspark.sql.DataFrame.filter\">\n",
    "<img align=left src=\"files/images/pyspark-pictures-dataframes-page22.svg\" width=500 height=500 />\n",
    "</a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# filter\n",
    "x = sqlContext.createDataFrame([('Alice',\"Bob\",0.1),(\"Bob\",\"Carol\",0.2),(\"Carol\",\"Dave\",0.3)], ['from','to','amt'])\n",
    "y = x.filter(\"amt > 0.1\")\n",
    "x.show()\n",
    "y.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a href=\"http://spark.apache.org/docs/latest/api/python/pyspark.sql.html#pyspark.sql.DataFrame.first\">\n",
    "<img align=left src=\"files/images/pyspark-pictures-dataframes-page23.svg\" width=500 height=500 />\n",
    "</a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----+-----+---+\n",
      "| from|   to|amt|\n",
      "+-----+-----+---+\n",
      "|Alice|  Bob|0.1|\n",
      "|  Bob|Carol|0.2|\n",
      "|Carol| Dave|0.3|\n",
      "+-----+-----+---+\n",
      "\n",
      "Row(from=u'Alice', to=u'Bob', amt=0.1)\n"
     ]
    }
   ],
   "source": [
    "# first\n",
    "x = sqlContext.createDataFrame([('Alice',\"Bob\",0.1),(\"Bob\",\"Carol\",0.2),(\"Carol\",\"Dave\",0.3)], ['from','to','amt'])\n",
    "y = x.first()\n",
    "x.show()\n",
    "print(y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a href=\"http://spark.apache.org/docs/latest/api/python/pyspark.sql.html#pyspark.sql.DataFrame.flatMap\">\n",
    "<img align=left src=\"files/images/pyspark-pictures-dataframes-page24.svg\" width=500 height=500 />\n",
    "</a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "'DataFrame' object has no attribute 'map'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-23-a44b4b53d662>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mx\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmap\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;32mlambda\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcollect\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m/home/dush/Spark/spark-2.1.0-bin-hadoop2.7/python/pyspark/sql/dataframe.pyc\u001b[0m in \u001b[0;36m__getattr__\u001b[0;34m(self, name)\u001b[0m\n\u001b[1;32m    962\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mname\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcolumns\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    963\u001b[0m             raise AttributeError(\n\u001b[0;32m--> 964\u001b[0;31m                 \"'%s' object has no attribute '%s'\" % (self.__class__.__name__, name))\n\u001b[0m\u001b[1;32m    965\u001b[0m         \u001b[0mjc\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_jdf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mapply\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    966\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mColumn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mjc\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mAttributeError\u001b[0m: 'DataFrame' object has no attribute 'map'"
     ]
    }
   ],
   "source": [
    "x.map(lambda x: (x[0], x[1])).collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "x.flatMap(lambda x: (x[0], x[1])).collect()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a href=\"http://spark.apache.org/docs/latest/api/python/pyspark.sql.html#pyspark.sql.DataFrame.foreach\">\n",
    "<img align=left src=\"files/images/pyspark-pictures-dataframes-page25.svg\" width=500 height=500 />\n",
    "</a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# foreach\n",
    "from __future__ import print_function\n",
    "\n",
    "# setup\n",
    "fn = './foreachExampleDataFrames.txt' \n",
    "open(fn, 'w').close()  # clear the file\n",
    "def fappend(el, f):\n",
    "    '''appends el to file f'''\n",
    "    print(el, file=open(f, 'a+') )\n",
    "\n",
    "# example\n",
    "x = sqlContext.createDataFrame([('Alice',\"Bob\",0.1),(\"Bob\",\"Carol\",0.2),(\"Carol\",\"Dave\",0.3)], ['from','to','amt'])\n",
    "y = x.foreach(lambda x: fappend(x, fn)) # writes into foreachExampleDataFrames.txt\n",
    "x.show() # original dataframe\n",
    "print(y) # foreach returns 'None'\n",
    "# print the contents of the file\n",
    "with open(fn, \"r\") as foreachExample:\n",
    "    print (foreachExample.read())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a href=\"http://spark.apache.org/docs/latest/api/python/pyspark.sql.html#pyspark.sql.DataFrame.foreachPartition\">\n",
    "<img align=left src=\"files/images/pyspark-pictures-dataframes-page26.svg\" width=500 height=500 />\n",
    "</a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# foreachPartition\n",
    "from __future__ import print_function\n",
    "\n",
    "#setup\n",
    "fn = './foreachPartitionExampleDataFrames.txt'\n",
    "open(fn, 'w').close()  # clear the file\n",
    "def fappend(partition,f):\n",
    "    '''append all elements in partition to file f'''\n",
    "    print([el for el in partition],file=open(f, 'a+'))\n",
    "\n",
    "x = sqlContext.createDataFrame([('Alice',\"Bob\",0.1),(\"Bob\",\"Carol\",0.2),(\"Carol\",\"Dave\",0.3)], ['from','to','amt'])\n",
    "x = x.repartition(2) # force 2 partitions\n",
    "y = x.foreachPartition(lambda x: fappend(x,fn)) # writes into foreachPartitionExampleDataFrames.txt\n",
    "\n",
    "x.show() # original dataframe\n",
    "print(y) # foreach returns 'None'\n",
    "# print the contents of the file\n",
    "with open(fn, \"r\") as foreachExample:\n",
    "    print (foreachExample.read())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a href=\"http://spark.apache.org/docs/latest/api/python/pyspark.sql.html#pyspark.sql.DataFrame.freqItems\">\n",
    "<img align=left src=\"files/images/pyspark-pictures-dataframes-page27.svg\" width=500 height=500 />\n",
    "</a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----+-----+---+\n",
      "| from|   to|amt|\n",
      "+-----+-----+---+\n",
      "|  Bob|Carol|0.1|\n",
      "|Alice| Dave|0.1|\n",
      "|Alice|  Bob|0.1|\n",
      "|Alice|  Bob|0.5|\n",
      "|Carol|  Bob|0.1|\n",
      "|Carol|  Bob|0.1|\n",
      "|Carol|  Bob|0.1|\n",
      "+-----+-----+---+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# freqItems\n",
    "x = sqlContext.createDataFrame([(\"Bob\",\"Carol\",0.1), \\\n",
    "                                (\"Alice\",\"Dave\",0.1), \\\n",
    "                                (\"Alice\",\"Bob\",0.1), \\\n",
    "                                (\"Alice\",\"Bob\",0.5), \\\n",
    "                                (\"Carol\",\"Bob\",0.1), \\\n",
    "                               (\"Carol\",\"Bob\",0.1), \\\n",
    "                               (\"Carol\",\"Bob\",0.1)], \\\n",
    "                               ['from','to','amt'])\n",
    "x.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----+-----+---+\n",
      "| from|   to|amt|\n",
      "+-----+-----+---+\n",
      "|  Bob|Carol|0.1|\n",
      "|Alice| Dave|0.1|\n",
      "|Alice|  Bob|0.1|\n",
      "|Alice|  Bob|0.5|\n",
      "|Carol|  Bob|0.1|\n",
      "|Carol|  Bob|0.1|\n",
      "|Carol|  Bob|0.1|\n",
      "+-----+-----+---+\n",
      "\n",
      "+--------------+-------------+\n",
      "|from_freqItems|amt_freqItems|\n",
      "+--------------+-------------+\n",
      "|[Alice, Carol]|   [0.1, 0.5]|\n",
      "+--------------+-------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "y = x.freqItems(cols=['from','amt'], support=0.4)\n",
    "x.show()\n",
    "y.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a href=\"http://spark.apache.org/docs/latest/api/python/pyspark.sql.html#pyspark.sql.DataFrame.groupBy\">\n",
    "<img align=left src=\"files/images/pyspark-pictures-dataframes-page28.svg\" width=500 height=500 />\n",
    "</a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# groupBy\n",
    "x = sqlContext.createDataFrame([('Alice',\"Bob\",0.1),(\"Alice\",\"Carol\",0.2),(\"Carol\",\"Dave\",0.3)], ['from','to','amt'])\n",
    "y = x.groupBy('from')\n",
    "x.show()\n",
    "print(y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a href=\"http://spark.apache.org/docs/latest/api/python/pyspark.sql.html#pyspark.sql.DataFrame.groupBy\">\n",
    "<img align=left src=\"files/images/pyspark-pictures-dataframes-page29.svg\" width=500 height=500 />\n",
    "</a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# groupBy(col1).avg(col2)\n",
    "x = sqlContext.createDataFrame([('Alice',\"Bob\",0.1),(\"Alice\",\"Carol\",0.2),(\"Carol\",\"Dave\",0.3)], ['from','to','amt'])\n",
    "y = x.groupBy('from').avg('amt')\n",
    "x.show()\n",
    "y.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a href=\"http://spark.apache.org/docs/latest/api/python/pyspark.sql.html#pyspark.sql.DataFrame.head\">\n",
    "<img align=left src=\"files/images/pyspark-pictures-dataframes-page30.svg\" width=500 height=500 />\n",
    "</a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# head\n",
    "x = sqlContext.createDataFrame([('Alice',\"Bob\",0.1),(\"Bob\",\"Carol\",0.2),(\"Carol\",\"Dave\",0.3)], ['from','to','amt'])\n",
    "y = x.head()\n",
    "x.show()\n",
    "print(y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a href=\"http://spark.apache.org/docs/latest/api/python/pyspark.sql.html#pyspark.sql.DataFrame.intersect\">\n",
    "<img align=left src=\"files/images/pyspark-pictures-dataframes-page31.svg\" width=500 height=500 />\n",
    "</a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----+-----+---+\n",
      "| from|   to|amt|\n",
      "+-----+-----+---+\n",
      "|Alice|  Bob|0.1|\n",
      "|  Bob|Carol|0.2|\n",
      "|Carol| Dave|0.3|\n",
      "+-----+-----+---+\n",
      "\n",
      "+-----+-----+---+\n",
      "| from|   to|amt|\n",
      "+-----+-----+---+\n",
      "|Alice|  Bob|0.1|\n",
      "|  Bob|Alice|0.2|\n",
      "|Carol| Dave|0.1|\n",
      "+-----+-----+---+\n",
      "\n",
      "+-----+---+---+\n",
      "| from| to|amt|\n",
      "+-----+---+---+\n",
      "|Alice|Bob|0.1|\n",
      "+-----+---+---+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# intersect\n",
    "x = sqlContext.createDataFrame([('Alice',\"Bob\",0.1),(\"Bob\",\"Carol\",0.2),(\"Carol\",\"Dave\",0.3)], ['from','to','amt'])\n",
    "y = sqlContext.createDataFrame([('Alice',\"Bob\",0.1),(\"Bob\",\"Alice\",0.2),(\"Carol\",\"Dave\",0.1)], ['from','to','amt'])\n",
    "z = x.intersect(y)\n",
    "x.show()\n",
    "y.show()\n",
    "z.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a href=\"http://spark.apache.org/docs/latest/api/python/pyspark.sql.html#pyspark.sql.DataFrame.isLocal\">\n",
    "<img align=left src=\"files/images/pyspark-pictures-dataframes-page32.svg\" width=500 height=500 />\n",
    "</a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----+-----+---+\n",
      "| from|   to|amt|\n",
      "+-----+-----+---+\n",
      "|Alice|  Bob|0.1|\n",
      "|  Bob|Carol|0.2|\n",
      "|Carol| Dave|0.3|\n",
      "+-----+-----+---+\n",
      "\n",
      "False\n"
     ]
    }
   ],
   "source": [
    "# isLocal\n",
    "x = sqlContext.createDataFrame([('Alice',\"Bob\",0.1),\n",
    "                                (\"Bob\",\"Carol\",0.2),(\"Carol\",\"Dave\",0.3)], \n",
    "                               ['from','to','amt'])\n",
    "y = x.isLocal()\n",
    "x.show()\n",
    "print(y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a href=\"http://spark.apache.org/docs/latest/api/python/pyspark.sql.html#pyspark.sql.DataFrame.join\">\n",
    "<img align=left src=\"files/images/pyspark-pictures-dataframes-page33.svg\" width=500 height=500 />\n",
    "</a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# join\n",
    "x = sqlContext.createDataFrame([('Alice',\"Bob\",0.1),(\"Bob\",\"Carol\",0.2),(\"Carol\",\"Dave\",0.3)], ['from','to','amt'])\n",
    "y = sqlContext.createDataFrame([('Alice',20),(\"Bob\",40),(\"Dave\",80)], ['name','age'])\n",
    "\n",
    "x.show()\n",
    "y.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "z = x.join(y, x.to == y.name,'inner').select('from','to','amt','age')\n",
    "z.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "x.join(y, x.to == y.name,'left').select('from','to','amt','age').show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a href=\"http://spark.apache.org/docs/latest/api/python/pyspark.sql.html#pyspark.sql.DataFrame.limit\">\n",
    "<img align=left src=\"files/images/pyspark-pictures-dataframes-page34.svg\" width=500 height=500 />\n",
    "</a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----+-----+---+\n",
      "| from|   to|amt|\n",
      "+-----+-----+---+\n",
      "|Alice|  Bob|0.1|\n",
      "|  Bob|Carol|0.2|\n",
      "|Carol| Dave|0.3|\n",
      "+-----+-----+---+\n",
      "\n",
      "+-----+-----+---+\n",
      "| from|   to|amt|\n",
      "+-----+-----+---+\n",
      "|Alice|  Bob|0.1|\n",
      "|  Bob|Carol|0.2|\n",
      "+-----+-----+---+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# limit\n",
    "x = sqlContext.createDataFrame([('Alice',\"Bob\",0.1),(\"Bob\",\"Carol\",0.2),(\"Carol\",\"Dave\",0.3)], ['from','to','amt'])\n",
    "y = x.limit(2)\n",
    "x.show()\n",
    "y.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a href=\"http://spark.apache.org/docs/latest/api/python/pyspark.sql.html#pyspark.sql.DataFrame.na\">\n",
    "<img align=left src=\"files/images/pyspark-pictures-dataframes-page37.svg\" width=500 height=500 />\n",
    "</a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----+-----+----+\n",
      "| from|   to| amt|\n",
      "+-----+-----+----+\n",
      "| null|  Bob| 0.1|\n",
      "|  Bob|Carol|null|\n",
      "|Carol| null| 0.3|\n",
      "|  Bob|Carol| 0.2|\n",
      "+-----+-----+----+\n",
      "\n",
      "<class 'pyspark.sql.dataframe.DataFrameNaFunctions'>\n",
      "<pyspark.sql.dataframe.DataFrameNaFunctions object at 0x7fce72019910>\n",
      "+----+-----+---+\n",
      "|from|   to|amt|\n",
      "+----+-----+---+\n",
      "| Bob|Carol|0.2|\n",
      "+----+-----+---+\n",
      "\n",
      "+-------+-------+---+\n",
      "|   from|     to|amt|\n",
      "+-------+-------+---+\n",
      "|unknown|    Bob|0.1|\n",
      "|    Bob|  Carol|0.0|\n",
      "|  Carol|missing|0.3|\n",
      "|    Bob|  Carol|0.2|\n",
      "+-------+-------+---+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# na\n",
    "x = sqlContext.createDataFrame([(None,\"Bob\",0.1),(\"Bob\",\"Carol\",None),(\"Carol\",None,0.3),(\"Bob\",\"Carol\",0.2)], ['from','to','amt'])\n",
    "y = x.na  # returns an object for handling missing values, supports drop, fill, and replace methods\n",
    "\n",
    "x.show()\n",
    "print type(y)\n",
    "print(y)\n",
    "\n",
    "y.drop().show()\n",
    "y.fill({'from':'unknown','to':'missing','amt':0}).show()\n",
    "\n",
    "\n",
    "# Using fill with a numeric parameter will replace numeric missings, with a string will replace string missings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----+-----+---+\n",
      "| from|   to|amt|\n",
      "+-----+-----+---+\n",
      "| null|  Bob|0.1|\n",
      "|  Bob|Carol|0.0|\n",
      "|Carol| null|0.3|\n",
      "|  Bob|Carol|0.2|\n",
      "+-----+-----+---+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "y.fill(0).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----+-----+----+\n",
      "| from|   to| amt|\n",
      "+-----+-----+----+\n",
      "|    a|  Bob| 0.1|\n",
      "|  Bob|Carol|null|\n",
      "|Carol|    a| 0.3|\n",
      "|  Bob|Carol| 0.2|\n",
      "+-----+-----+----+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "y.fill('a').show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a href=\"http://spark.apache.org/docs/latest/api/python/pyspark.sql.html#pyspark.sql.DataFrame.orderBy\">\n",
    "<img align=left src=\"files/images/pyspark-pictures-dataframes-page38.svg\" width=500 height=500 />\n",
    "</a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# orderBy\n",
    "x = sqlContext.createDataFrame([('Alice',\"Bob\",0.1),(\"Bob\",\"Carol\",0.2),(\"Carol\",\"Dave\",0.3)], ['from','to','amt'])\n",
    "y = x.orderBy(['from'], ascending=[False])\n",
    "x.show()\n",
    "y.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a href=\"http://spark.apache.org/docs/latest/api/python/pyspark.sql.html#pyspark.sql.DataFrame.persist\">\n",
    "<img align=left src=\"files/images/pyspark-pictures-dataframes-page39.svg\" width=500 height=500 />\n",
    "</a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "x.persist?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----+-----+---+\n",
      "| from|   to|amt|\n",
      "+-----+-----+---+\n",
      "|Alice|  Bob|0.1|\n",
      "|  Bob|Carol|0.2|\n",
      "|Carol| Dave|0.3|\n",
      "+-----+-----+---+\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# persist\n",
    "x = sqlContext.createDataFrame([('Alice',\"Bob\",0.1),(\"Bob\",\"Carol\",0.2),(\"Carol\",\"Dave\",0.3)], ['from','to','amt'])\n",
    "x.persist(storageLevel=StorageLevel(True,True,False,True,1)) \n",
    "# StorageLevel(useDisk, useMemory, useOffHeap, deserialized, replication=1)\n",
    "\n",
    "x.show()\n",
    "x.is_cached"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a href=\"http://spark.apache.org/docs/latest/api/python/pyspark.sql.html#pyspark.sql.DataFrame.printSchema\">\n",
    "<img align=left src=\"files/images/pyspark-pictures-dataframes-page40.svg\" width=500 height=500 />\n",
    "</a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# printSchema\n",
    "x = sqlContext.createDataFrame([('Alice',\"Bob\",0.1),(\"Bob\",\"Carol\",0.2),(\"Carol\",\"Dave\",0.3)], ['from','to','amt'])\n",
    "x.show()\n",
    "x.printSchema()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a href=\"http://spark.apache.org/docs/latest/api/python/pyspark.sql.html#pyspark.sql.DataFrame.randomSplit\">\n",
    "<img align=left src=\"files/images/pyspark-pictures-dataframes-page41.svg\" width=500 height=500 />\n",
    "</a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "list"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# randomSplit\n",
    "x = sqlContext.createDataFrame([('Alice',\"Bob\",0.1),(\"Bob\",\"Carol\",0.2),(\"Carol\",\"Dave\",0.3)], ['from','to','amt'])\n",
    "y = x.randomSplit([0.5, 0.5])\n",
    "type(y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----+-----+---+\n",
      "| from|   to|amt|\n",
      "+-----+-----+---+\n",
      "|Alice|  Bob|0.1|\n",
      "|  Bob|Carol|0.2|\n",
      "+-----+-----+---+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "y[0].show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----+----+---+\n",
      "| from|  to|amt|\n",
      "+-----+----+---+\n",
      "|Carol|Dave|0.3|\n",
      "+-----+----+---+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "y[1].show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[19, 16, 15]"
      ]
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "[x.count() for x in sdf_x.randomSplit([0.3, 0.3, 0.4])]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----+-----+---+\n",
      "| from|   to|amt|\n",
      "+-----+-----+---+\n",
      "|Alice|  Bob|0.1|\n",
      "|  Bob|Carol|0.2|\n",
      "+-----+-----+---+\n",
      "\n",
      "+-----+----+---+\n",
      "| from|  to|amt|\n",
      "+-----+----+---+\n",
      "|Carol|Dave|0.3|\n",
      "+-----+----+---+\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[None, None]"
      ]
     },
     "execution_count": 48,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "[i.show() for i in y]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "sdf_x.rdd.take(2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a href=\"http://spark.apache.org/docs/latest/api/python/pyspark.sql.html#pyspark.sql.DataFrame.rdd\">\n",
    "<img align=left src=\"files/images/pyspark-pictures-dataframes-page42.svg\" width=500 height=500 />\n",
    "</a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----+-----+---+\n",
      "| from|   to|amt|\n",
      "+-----+-----+---+\n",
      "|Alice|  Bob|0.1|\n",
      "|  Bob|Carol|0.2|\n",
      "|Carol| Dave|0.3|\n",
      "+-----+-----+---+\n",
      "\n",
      "[Row(from=u'Alice', to=u'Bob', amt=0.1), Row(from=u'Bob', to=u'Carol', amt=0.2), Row(from=u'Carol', to=u'Dave', amt=0.3)]\n"
     ]
    }
   ],
   "source": [
    "# rdd\n",
    "x = sqlContext.createDataFrame([('Alice',\"Bob\",0.1),(\"Bob\",\"Carol\",0.2),(\"Carol\",\"Dave\",0.3)], ['from','to','amt'])\n",
    "y = x.rdd\n",
    "x.show()\n",
    "print(y.collect())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a href=\"http://spark.apache.org/docs/latest/api/python/pyspark.sql.html#pyspark.sql.DataFrame.registerTempTable\">\n",
    "<img align=left src=\"files/images/pyspark-pictures-dataframes-page43.svg\" width=500 height=500 />\n",
    "</a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----+-----+---+\n",
      "| from|   to|amt|\n",
      "+-----+-----+---+\n",
      "|Alice|  Bob|0.1|\n",
      "|  Bob|Carol|0.2|\n",
      "|Carol| Dave|0.3|\n",
      "+-----+-----+---+\n",
      "\n",
      "+-----+-----+---+\n",
      "| from|   to|amt|\n",
      "+-----+-----+---+\n",
      "|  Bob|Carol|0.2|\n",
      "|Carol| Dave|0.3|\n",
      "+-----+-----+---+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# registerTempTable\n",
    "x = sqlContext.createDataFrame([('Alice',\"Bob\",0.1),(\"Bob\",\"Carol\",0.2),(\"Carol\",\"Dave\",0.3)], ['from','to','amt'])\n",
    "x.registerTempTable(name=\"TRANSACTIONS\")\n",
    "y = sqlContext.sql('SELECT * FROM TRANSACTIONS WHERE amt > 0.1')\n",
    "x.show()\n",
    "y.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a href=\"http://spark.apache.org/docs/latest/api/python/pyspark.sql.html#pyspark.sql.DataFrame.repartition\">\n",
    "<img align=left src=\"files/images/pyspark-pictures-dataframes-page44.svg\" width=500 height=500 />\n",
    "</a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4\n",
      "3\n"
     ]
    }
   ],
   "source": [
    "# repartition\n",
    "x = sdf_x\n",
    "y = x.repartition(3)\n",
    "print(x.rdd.getNumPartitions())\n",
    "print(y.rdd.getNumPartitions())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a href=\"http://spark.apache.org/docs/latest/api/python/pyspark.sql.html#pyspark.sql.DataFrame.replace\">\n",
    "<img align=left src=\"files/images/pyspark-pictures-dataframes-page45.svg\" width=500 height=500 />\n",
    "</a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----+-----+---+\n",
      "| from|   to|amt|\n",
      "+-----+-----+---+\n",
      "| Dave|  Bob|0.1|\n",
      "|  Bob|Carol|0.2|\n",
      "|Carol| Dave|0.3|\n",
      "+-----+-----+---+\n",
      "\n",
      "+-----+-----+---+\n",
      "| from|   to|amt|\n",
      "+-----+-----+---+\n",
      "|David|  Bob|0.1|\n",
      "|  Bob|Carol|0.2|\n",
      "|Carol|David|0.3|\n",
      "+-----+-----+---+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# replace\n",
    "x = sqlContext.createDataFrame([('Dave',\"Bob\",0.1),(\"Bob\",\"Carol\",0.2),(\"Carol\",\"Dave\",0.3)], ['from','to','amt'])\n",
    "y = x.replace('Dave','David',['from','to'])\n",
    "x.show()\n",
    "y.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a href=\"http://spark.apache.org/docs/latest/api/python/pyspark.sql.html#pyspark.sql.DataFrame.rollup\">\n",
    "<img align=left src=\"files/images/pyspark-pictures-dataframes-page46.svg\" width=500 height=500 />\n",
    "</a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----+-----+---+\n",
      "| from|   to|amt|\n",
      "+-----+-----+---+\n",
      "|Alice|  Bob|0.1|\n",
      "|  Bob|Carol|0.2|\n",
      "|Carol| Dave|0.3|\n",
      "+-----+-----+---+\n",
      "\n",
      "<pyspark.sql.group.GroupedData object at 0x7fce71f9e310>\n",
      "+-----+-----+------------------+\n",
      "| from|   to|          sum(amt)|\n",
      "+-----+-----+------------------+\n",
      "| null| null|0.6000000000000001|\n",
      "|  Bob|Carol|               0.2|\n",
      "|Carol| null|               0.3|\n",
      "|Alice|  Bob|               0.1|\n",
      "|  Bob| null|               0.2|\n",
      "|Carol| Dave|               0.3|\n",
      "|Alice| null|               0.1|\n",
      "+-----+-----+------------------+\n",
      "\n",
      "+-----+-----+--------+\n",
      "| from|   to|max(amt)|\n",
      "+-----+-----+--------+\n",
      "| null| null|     0.3|\n",
      "|  Bob|Carol|     0.2|\n",
      "|Carol| null|     0.3|\n",
      "|Alice|  Bob|     0.1|\n",
      "|  Bob| null|     0.2|\n",
      "|Carol| Dave|     0.3|\n",
      "|Alice| null|     0.1|\n",
      "+-----+-----+--------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# rollup\n",
    "x = sqlContext.createDataFrame([('Alice',\"Bob\",0.1),(\"Bob\",\"Carol\",0.2),(\"Carol\",\"Dave\",0.3)], ['from','to','amt'])\n",
    "y = x.rollup(['from','to'])\n",
    "x.show()\n",
    "print(y) # y is a grouped data object, aggregations will be applied to all numerical columns\n",
    "y.sum().show()\n",
    "y.max().show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a href=\"http://spark.apache.org/docs/latest/api/python/pyspark.sql.html#pyspark.sql.DataFrame.sample\">\n",
    "<img align=left src=\"files/images/pyspark-pictures-dataframes-page47.svg\" width=500 height=500 />\n",
    "</a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# sample\n",
    "\n",
    "y = sdf_x.sample(False, 0.1)\n",
    "sdf_x.show(5)\n",
    "y.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a href=\"http://spark.apache.org/docs/latest/api/python/pyspark.sql.html#pyspark.sql.DataFrame.sampleBy\">\n",
    "<img align=left src=\"files/images/pyspark-pictures-dataframes-page48.svg\" width=500 height=500 />\n",
    "</a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Used for Stratified Sampling\n",
    "\n",
    "# sampleBy\n",
    "x = sqlContext.createDataFrame([('Alice',\"Bob\",0.1),(\"Alice\",\"Carol\",0.2),(\"Alice\",\"Alice\",0.3), \\\n",
    "                               ('Alice',\"Dave\",0.4),(\"Bob\",\"Bob\",0.5),(\"Bob\",\"Carol\",0.6)], \\\n",
    "                                ['from','to','amt'])\n",
    "y = x.sampleBy(col='from',fractions={'Alice':0.1,'Bob':0.9})\n",
    "x.show()\n",
    "y.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a href=\"http://spark.apache.org/docs/latest/api/python/pyspark.sql.html#pyspark.sql.DataFrame.schema>\n",
    "<img align=left src=\"files/images/pyspark-pictures-dataframes-page49.svg\" width=500 height=500 />\n",
    "</a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# schema\n",
    "x = sqlContext.createDataFrame([('Alice',\"Bob\",0.1),(\"Bob\",\"Carol\",0.2),(\"Carol\",\"Dave\",0.3)], ['from','to','amt'])\n",
    "y = x.schema\n",
    "x.show()\n",
    "print(y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a href=\"http://spark.apache.org/docs/latest/api/python/pyspark.sql.html#pyspark.sql.DataFrame.select\">\n",
    "<img align=left src=\"files/images/pyspark-pictures-dataframes-page50.svg\" width=500 height=500 />\n",
    "</a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# select\n",
    "x = sqlContext.createDataFrame([('Alice',\"Bob\",0.1),(\"Bob\",\"Carol\",0.2),(\"Carol\",\"Dave\",0.3)], ['from','to','amt'])\n",
    "y = x.select(['from','amt'])\n",
    "x.show()\n",
    "y.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a href=\"http://spark.apache.org/docs/latest/api/python/pyspark.sql.html#pyspark.sql.DataFrame.selectExpr\">\n",
    "<img align=left src=\"files/images/pyspark-pictures-dataframes-page51.svg\" width=500 height=500 />\n",
    "</a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----+-----+---+\n",
      "| from|   to|amt|\n",
      "+-----+-----+---+\n",
      "|Alice|  Bob|0.1|\n",
      "|  Bob|Carol|0.2|\n",
      "|Carol| Dave|0.3|\n",
      "+-----+-----+---+\n",
      "\n",
      "+---------------------+----------+\n",
      "|substring(from, 1, 1)|(amt + 10)|\n",
      "+---------------------+----------+\n",
      "|                    A|      10.1|\n",
      "|                    B|      10.2|\n",
      "|                    C|      10.3|\n",
      "+---------------------+----------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# selectExpr\n",
    "x = sqlContext.createDataFrame([('Alice',\"Bob\",0.1),(\"Bob\",\"Carol\",0.2),(\"Carol\",\"Dave\",0.3)], ['from','to','amt'])\n",
    "y = x.selectExpr(['substr(from,1,1)','amt+10'])\n",
    "x.show()\n",
    "y.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a href=\"http://spark.apache.org/docs/latest/api/python/pyspark.sql.html#pyspark.sql.DataFrame.show\">\n",
    "<img align=left src=\"files/images/pyspark-pictures-dataframes-page52.svg\" width=500 height=500 />\n",
    "</a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# show\n",
    "x = sqlContext.createDataFrame([('Alice',\"Bob\",0.1),(\"Bob\",\"Carol\",0.2),(\"Carol\",\"Dave\",0.3)], ['from','to','amt'])\n",
    "x.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a href=\"http://spark.apache.org/docs/latest/api/python/pyspark.sql.html#pyspark.sql.DataFrame.sort\">\n",
    "<img align=left src=\"files/images/pyspark-pictures-dataframes-page53.svg\" width=500 height=500 />\n",
    "</a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# sort\n",
    "x = sqlContext.createDataFrame([('Alice',\"Bob\",0.1),(\"Bob\",\"Carol\",0.2),(\"Carol\",\"Alice\",0.3)], ['from','to','amt'])\n",
    "y = x.sort(['to'])\n",
    "x.show()\n",
    "y.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a href=\"http://spark.apache.org/docs/latest/api/python/pyspark.sql.html#pyspark.sql.DataFrame.stat\">\n",
    "<img align=left src=\"files/images/pyspark-pictures-dataframes-page55.svg\" width=500 height=500 />\n",
    "</a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# stat\n",
    "x = sqlContext.createDataFrame([(\"Alice\",\"Bob\",0.1,0.001),(\"Bob\",\"Carol\",0.2,0.02),(\"Carol\",\"Dave\",0.3,0.02)], ['from','to','amt','fee'])\n",
    "y = x.stat\n",
    "x.show()\n",
    "print(y)\n",
    "print(y.corr(col1=\"amt\",col2=\"fee\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a href=\"http://spark.apache.org/docs/latest/api/python/pyspark.sql.html#pyspark.sql.DataFrame.subtract\">\n",
    "<img align=left src=\"files/images/pyspark-pictures-dataframes-page56.svg\" width=500 height=500 />\n",
    "</a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# subtract\n",
    "x = sqlContext.createDataFrame([('Alice',\"Bob\",0.1),(\"Bob\",\"Carol\",0.2),(\"Carol\",\"Dave\",0.3)], ['from','to','amt'])\n",
    "y = sqlContext.createDataFrame([('Alice',\"Bob\",0.1),(\"Bob\",\"Carol\",0.2),(\"Carol\",\"Dave\",0.1)], ['from','to','amt'])\n",
    "z = x.subtract(y)\n",
    "x.show()\n",
    "y.show()\n",
    "z.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a href=\"http://spark.apache.org/docs/latest/api/python/pyspark.sql.html#pyspark.sql.DataFrame.take\">\n",
    "<img align=left src=\"files/images/pyspark-pictures-dataframes-page57.svg\" width=500 height=500 />\n",
    "</a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# take\n",
    "x = sqlContext.createDataFrame([('Alice',\"Bob\",0.1),(\"Bob\",\"Carol\",0.2),(\"Carol\",\"Dave\",0.3)], ['from','to','amt'])\n",
    "y = x.take(num=2)\n",
    "x.show()\n",
    "print(y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a href=\"http://spark.apache.org/docs/latest/api/python/pyspark.sql.html#pyspark.sql.DataFrame.toDF\">\n",
    "<img align=left src=\"files/images/pyspark-pictures-dataframes-page58.svg\" width=500 height=500 />\n",
    "</a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+---+\n",
      "|  A|  B|\n",
      "+---+---+\n",
      "|  1|  2|\n",
      "|  3|  4|\n",
      "|  5|  6|\n",
      "+---+---+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# toDF\n",
    "x = sc.parallelize([[1, 2],[3, 4],[5, 6]])\n",
    "y = x.toDF([\"A\", \"B\"])\n",
    "y.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a href=\"http://spark.apache.org/docs/latest/api/python/pyspark.sql.html#pyspark.sql.DataFrame.toJSON\">\n",
    "<img align=left src=\"files/images/pyspark-pictures-dataframes-page59.svg\" width=500 height=500 />\n",
    "</a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----+-----+---+\n",
      "| from|   to|amt|\n",
      "+-----+-----+---+\n",
      "|Alice|  Bob|0.1|\n",
      "|  Bob|Carol|0.2|\n",
      "|Carol|Alice|0.3|\n",
      "+-----+-----+---+\n",
      "\n",
      "[u'{\"from\":\"Alice\",\"to\":\"Bob\",\"amt\":0.1}', u'{\"from\":\"Bob\",\"to\":\"Carol\",\"amt\":0.2}', u'{\"from\":\"Carol\",\"to\":\"Alice\",\"amt\":0.3}']\n"
     ]
    }
   ],
   "source": [
    "# toJSON\n",
    "x = sqlContext.createDataFrame([('Alice',\"Bob\",0.1),(\"Bob\",\"Carol\",0.2),(\"Carol\",\"Alice\",0.3)], ['from','to','amt'])\n",
    "y = x.toJSON()\n",
    "x.show()\n",
    "print(y.collect())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a href=\"http://spark.apache.org/docs/latest/api/python/pyspark.sql.html#pyspark.sql.DataFrame.toPandas\">\n",
    "<img align=left src=\"files/images/pyspark-pictures-dataframes-page60.svg\" width=500 height=500 />\n",
    "</a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# toPandas\n",
    "x = sqlContext.createDataFrame([('Alice',\"Bob\",0.1),(\"Bob\",\"Carol\",0.2),(\"Carol\",\"Dave\",0.3)], ['from','to','amt'])\n",
    "y = x.toPandas()\n",
    "x.show()\n",
    "print(type(y))\n",
    "y"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a href=\"http://spark.apache.org/docs/latest/api/python/pyspark.sql.html#pyspark.sql.DataFrame.unionAll\">\n",
    "<img align=left src=\"files/images/pyspark-pictures-dataframes-page61.svg\" width=500 height=500 />\n",
    "</a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# unionAll\n",
    "x = sqlContext.createDataFrame([('Alice',\"Bob\",0.1),(\"Bob\",\"Carol\",0.2)], ['from','to','amt'])\n",
    "y = sqlContext.createDataFrame([(\"Bob\",\"Carol\",0.2),(\"Carol\",\"Dave\",0.1)], ['from','to','amt'])\n",
    "z = x.unionAll(y)\n",
    "x.show()\n",
    "y.show()\n",
    "z.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a href=\"http://spark.apache.org/docs/latest/api/python/pyspark.sql.html#pyspark.sql.DataFrame.unpersist\">\n",
    "<img align=left src=\"files/images/pyspark-pictures-dataframes-page62.svg\" width=500 height=500 />\n",
    "</a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# unpersist\n",
    "x = sqlContext.createDataFrame([('Alice',\"Bob\",0.1),(\"Bob\",\"Carol\",0.2),(\"Carol\",\"Dave\",0.3)], ['from','to','amt'])\n",
    "x.cache()\n",
    "x.count()\n",
    "x.show()\n",
    "print(x.is_cached)\n",
    "x.unpersist()\n",
    "print(x.is_cached)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a href=\"http://spark.apache.org/docs/latest/api/python/pyspark.sql.html#pyspark.sql.DataFrame.where\">\n",
    "<img align=left src=\"files/images/pyspark-pictures-dataframes-page63.svg\" width=500 height=500 />\n",
    "</a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# where (filter)\n",
    "x = sqlContext.createDataFrame([('Alice',\"Bob\",0.1),(\"Bob\",\"Carol\",0.2),(\"Carol\",\"Dave\",0.3)], ['from','to','amt'])\n",
    "y = x.where(\"amt > 0.1\")\n",
    "x.show()\n",
    "y.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a href=\"http://spark.apache.org/docs/latest/api/python/pyspark.sql.html#pyspark.sql.DataFrame.withColumn\">\n",
    "<img align=left src=\"files/images/pyspark-pictures-dataframes-page64.svg\" width=500 height=500 />\n",
    "</a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# withColumn\n",
    "x = sqlContext.createDataFrame([('Alice',\"Bob\",0.1),(\"Bob\",\"Carol\",None),(\"Carol\",\"Dave\",0.3)], ['from','to','amt'])\n",
    "y = x.withColumn('conf',x.amt.isNotNull())\n",
    "x.show()\n",
    "y.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a href=\"http://spark.apache.org/docs/latest/api/python/pyspark.sql.html#pyspark.sql.DataFrame.withColumnRenamed\">\n",
    "<img align=left src=\"files/images/pyspark-pictures-dataframes-page65.svg\" width=500 height=500 />\n",
    "</a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# withColumnRenamed\n",
    "x = sqlContext.createDataFrame([('Alice',\"Bob\",0.1),(\"Bob\",\"Carol\",0.2),(\"Carol\",\"Dave\",0.3)], ['from','to','amt'])\n",
    "y = x.withColumnRenamed('amt','amount')\n",
    "x.show()\n",
    "y.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a href=\"http://spark.apache.org/docs/latest/api/python/pyspark.sql.html#pyspark.sql.DataFrame.write\">\n",
    "<img align=left src=\"files/images/pyspark-pictures-dataframes-page66.svg\" width=500 height=500 />\n",
    "</a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "[x for x in os.listdir(os.getcwd()) if 'json' in x]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# write\n",
    "import json\n",
    "# x = sqlContext.createDataFrame([('Alice',\"Bob\",0.1),(\"Bob\",\"Carol\",0.2),(\"Carol\",\"Dave\",0.3)], ['from','to','amt'])\n",
    "y2 = y.write.mode('overwrite').json('/Users/lr/Desktop/dataframeWriteExample.json')\n",
    "# x.show()\n",
    "# read the dataframe back in from file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "y = sdf_x.repartition(8)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "!head /Users/lr/Desktop/dataframeWriteExample.json/part-r-00000-75a53aa3-adba-4e61-a6a4-80ede8942b28.json"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "sqlContext.read.json('./dataframeWriteExample.json').show()"
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
