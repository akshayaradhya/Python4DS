{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Pipelines"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If we learn something so far, that is machine learning is not only applying an algorithm and get the predictions.(Not so much commodization after all) It has quite a lot of different and moving parts for a given problem.\n",
    "\n",
    "> Steps(feature extraction, feature selection, classifier, evaluation) follows a sequential order, though. \n",
    "\n",
    "- Would it be perfect if we could wrap all of the steps in one object and then do the parameter search(i.e. grid parameter search) for cross validation in that object. \n",
    "- Further, if we have two estimators in the __pipeline__(say we apply PCA to reduce dimension in the input and then apply SVM), we would need only once to use the `fit` function in the estimator. \n",
    "\n",
    "> Pipeline automatically applies the correct steps for you to get the correct output at the end of the pipeline. \n",
    "\n",
    "- Still not convinced? What if I say, serializing one `pipeline` instead of serializing `vectorizer`, `feature_selector` and `classifier` separately and then deploying into production makes much easier.(More on to this in the next notebook) This was a quick win for the pipeline. Let's see how one might use it in classification."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Why do we need pipelines\n",
    "\n",
    "- The basic API requires us to go through a series of steps common to most analyses\n",
    "- We need to perform the same things if we build a slightly different model\n",
    "- Parameter optimizations need to be handled separately for preprocessing steps and for model selection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from sklearn.datasets import load_boston\n",
    "from pandas import Series, DataFrame\n",
    "bos = load_boston()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "bos.keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "X = DataFrame(bos['data'], columns = bos['feature_names'])\n",
    "y = bos['target']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "X.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Regular Method of Building Models is to go step-by-step <br> and use the output of one step as input of the next step"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Step 1 - Scaling\n",
    "# From x to x_scaled\n",
    "\n",
    "from sklearn.preprocessing import RobustScaler\n",
    "robsc = RobustScaler() \n",
    "X_scaled = robsc.fit_transform(X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Step 2\n",
    "# from x_scaled to x_pca\n",
    "\n",
    "from sklearn.decomposition import PCA\n",
    "pca_10 = PCA(n_components=10, whiten=True)\n",
    "\n",
    "X_pca = pca_10.fit_transform(X_scaled)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Step 3\n",
    "# build the model\n",
    "\n",
    "from sklearn.svm import SVR\n",
    "from sklearn.grid_search import GridSearchCV\n",
    "\n",
    "grid = {'C': [0.01, 0.1, 1, 10],\n",
    "       'kernel': ['linear', 'rbf', 'poly']}\n",
    "\n",
    "gscv_svr = GridSearchCV(estimator=SVR(), param_grid=grid, cv=5, verbose=True)\n",
    "\n",
    "gscv_svr.fit(X_pca, y)\n",
    "\n",
    "gscv_svr.best_estimator_.predict(X_pca)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "Some might find this process needlessly verbose.\n",
    "There is a way to limit the amount of typing that needs to be done in order to accomplish the same tasks.\n",
    "\n",
    "> ### Pipeline can be used to chain multiple estimators/transformers into one. \n",
    "\n",
    "This is useful as there is often a **fixed sequence of steps in processing the data**, for example \n",
    "\n",
    "- feature selection, \n",
    "- normalization and \n",
    "- classification. \n",
    "\n",
    "Pipeline serves two purposes here:\n",
    "\n",
    "- **Convenience**: You only have to call fit and predict once on your data to fit a whole sequence of estimators.\n",
    "- **Joint parameter selection**: You can grid search over parameters of all estimators in the pipeline at once.\n",
    "\n",
    "> #### [PS] All estimators in a pipeline, except the last one, must be transformers (i.e. must have a transform method). The last estimator may be any type (transformer, classifier, etc.).\n",
    "\n",
    "\n",
    "The purpose of the pipeline is to assemble several steps that can be\n",
    "cross-validated together while setting different parameters.\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Build a basic Pipeline (without GridSearch)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from sklearn.pipeline import Pipeline, make_pipeline\n",
    "from sklearn.linear_model import LinearRegression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Give names of steps with make_pipeline\n",
    "pipe_1 = make_pipeline(RobustScaler(), PCA(n_components=10), LinearRegression())\n",
    "\n",
    "# Give names + aliases in tuples with Pipeline()\n",
    "pipe_2 = Pipeline([('scale', RobustScaler()), \n",
    "                  ('pca', PCA(n_components=10)), \n",
    "                  ('lr', LinearRegression())])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "pipe_2.fit(X, y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "pipe_2.predict(X)[:10]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Pipelines with GridSearch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Regular Grid Search for SVM with preprocessing using PCA \n",
    "\n",
    "from sklearn.decomposition import PCA\n",
    "\n",
    "pca = PCA(n_components=10)\n",
    "X_pca = pca.fit_transform(X)\n",
    "\n",
    "\n",
    "from sklearn.grid_search import GridSearchCV\n",
    "param_grid = {'C': 10. ** np.arange(-3, 3),\n",
    "              'gamma': 10. ** np.arange(-3, 3)}\n",
    "\n",
    "grid = GridSearchCV(SVC(), \n",
    "                    param_grid=param_grid,\n",
    "                    verbose=True,\n",
    "                    scoring='accuracy'\n",
    "                    n_jobs=-1)\n",
    "\n",
    "grid.fit(X_pca, y_train)\n",
    "grid.predict(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Combine them\n",
    "import numpy as np\n",
    "from sklearn.svm import SVC\n",
    "\n",
    "pipe_svm = Pipeline([('pca', PCA()), \n",
    "                     ('svm', SVC())])\n",
    "\n",
    "param_grid = {'pca__n_components': [5, 10, 15],\n",
    "              'svm__C': 10. ** np.arange(-3, 3), \n",
    "              'svm__gamma': 10. ** np.arange(-3, 3)\n",
    "             }\n",
    "\n",
    "grid = GridSearchCV(pipe_svm, \n",
    "                    param_grid=param_grid, \n",
    "                    cv=5,\n",
    "                   scoring='recall',\n",
    "                   n_jobs=-1)\n",
    "\n",
    "grid.fit(X, y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Example"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from sklearn.datasets import load_digits\n",
    "X = load_digits()['data']\n",
    "y = load_digits()['target']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from sklearn.cross_validation import train_test_split\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, train_size=0.8)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.preprocessing import  StandardScaler\n",
    "from sklearn.feature_selection import SelectKBest\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.grid_search import GridSearchCV\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.decomposition import PCA"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Pipeline for Logistic Regression with Scaling and PCA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# 1 - Set up the pipeline\n",
    "pipe_logit = Pipeline(steps=[('scale', StandardScaler()), \n",
    "                             ('pca', PCA()),\n",
    "                             ('logit', LogisticRegression())])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# 2 - Set up the pipeline grid\n",
    "params_pipe_logit = {'logit__C': [10, 1, 0.1, 0.01],\n",
    "                     'logit__penalty': ['l1', 'l2'],\n",
    "                     'pca__n_components': [20, 30, 40]}\n",
    "\n",
    "# 3 - Set up the grid search by passing the pipe and pipe grid\n",
    "gscv_pipe_1 = GridSearchCV(estimator=pipe_logit, \n",
    "                           param_grid=params_pipe_logit, \n",
    "                           scoring='accuracy',\n",
    "                           cv=3,\n",
    "                           verbose=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 3 folds for each of 24 candidates, totalling 72 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=1)]: Done  49 tasks       | elapsed:   12.1s\n",
      "[Parallel(n_jobs=1)]: Done  72 out of  72 | elapsed:   13.3s finished\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "GridSearchCV(cv=3, error_score='raise',\n",
       "       estimator=Pipeline(steps=[('scale', StandardScaler(copy=True, with_mean=True, with_std=True)), ('pca', PCA(copy=True, n_components=None, whiten=False)), ('logit', LogisticRegression(C=1.0, class_weight=None, dual=False, fit_intercept=True,\n",
       "          intercept_scaling=1, max_iter=100, multi_class='ovr', n_jobs=1,\n",
       "          penalty='l2', random_state=None, solver='liblinear', tol=0.0001,\n",
       "          verbose=0, warm_start=False))]),\n",
       "       fit_params={}, iid=True, n_jobs=1,\n",
       "       param_grid={'pca__n_components': [20, 30, 40], 'logit__penalty': ['l1', 'l2'], 'logit__C': [10, 1, 0.1, 0.01]},\n",
       "       pre_dispatch='2*n_jobs', refit=True, scoring='accuracy', verbose=1)"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 4 - Use the fit, predict methods of the grid\n",
    "\n",
    "# Fit\n",
    "gscv_pipe_1.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'pca__n_components': 40, 'logit__penalty': 'l2', 'logit__C': 1}\n"
     ]
    }
   ],
   "source": [
    "# Best Params\n",
    "print gscv_pipe_1.best_params_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Predict\n",
    "y_pred = gscv_pipe_1.predict(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.94444444444444442"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Measure Performance\n",
    "accuracy_score(y_pred, y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[mean: 0.93041, std: 0.00362, params: {'pca__n_components': 20, 'logit__penalty': 'l1', 'logit__C': 10},\n",
       " mean: 0.94224, std: 0.00584, params: {'pca__n_components': 30, 'logit__penalty': 'l1', 'logit__C': 10},\n",
       " mean: 0.94224, std: 0.00816, params: {'pca__n_components': 40, 'logit__penalty': 'l1', 'logit__C': 10},\n",
       " mean: 0.93180, std: 0.00823, params: {'pca__n_components': 20, 'logit__penalty': 'l2', 'logit__C': 10}]"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "gscv_pipe_1.grid_scores_[:4]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## Pipeline for SVM with Scaling and SelectKBest"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from sklearn.feature_selection import SelectKBest"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 3 folds for each of 48 candidates, totalling 144 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/lr/anaconda2/lib/python2.7/site-packages/sklearn/feature_selection/univariate_selection.py:113: UserWarning: Features [ 0 24 32 39] are constant.\n",
      "  UserWarning)\n",
      "/Users/lr/anaconda2/lib/python2.7/site-packages/sklearn/feature_selection/univariate_selection.py:114: RuntimeWarning: invalid value encountered in divide\n",
      "  f = msb / msw\n",
      "/Users/lr/anaconda2/lib/python2.7/site-packages/sklearn/feature_selection/univariate_selection.py:113: UserWarning: Features [ 0 32 39 56] are constant.\n",
      "  UserWarning)\n",
      "/Users/lr/anaconda2/lib/python2.7/site-packages/sklearn/feature_selection/univariate_selection.py:113: UserWarning: Features [ 0 32 39] are constant.\n",
      "  UserWarning)\n",
      "/Users/lr/anaconda2/lib/python2.7/site-packages/sklearn/metrics/classification.py:1074: UndefinedMetricWarning: F-score is ill-defined and being set to 0.0 in labels with no predicted samples.\n",
      "  'precision', 'predicted', average, warn_for)\n",
      "[Parallel(n_jobs=1)]: Done  49 tasks       | elapsed:    5.5s\n",
      "[Parallel(n_jobs=1)]: Done 144 out of 144 | elapsed:   15.8s finished\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "GridSearchCV(cv=3, error_score='raise',\n",
       "       estimator=Pipeline(steps=[('scale', StandardScaler(copy=True, with_mean=True, with_std=True)), ('skb', SelectKBest(k=10, score_func=<function f_classif at 0x1071aae60>)), ('svm', SVC(C=1.0, cache_size=200, class_weight=None, coef0=0.0,\n",
       "  decision_function_shape=None, degree=3, gamma='auto', kernel='rbf',\n",
       "  max_iter=-1, probability=False, random_state=None, shrinking=True,\n",
       "  tol=0.001, verbose=False))]),\n",
       "       fit_params={}, iid=True, n_jobs=1,\n",
       "       param_grid={'svm__C': array([  0.01,   0.1 ,   1.  ,  10.  ]), 'svm__kernel': ['linear', 'rbf', 'sigmoid'], 'skb__k': [20, 25, 35, 50]},\n",
       "       pre_dispatch='2*n_jobs', refit=True, scoring='f1_weighted',\n",
       "       verbose=1)"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Set up the pipe\n",
    "pipe_rf = Pipeline(steps=[('scale', StandardScaler()), \n",
    "                          ('skb', SelectKBest()),\n",
    "                          ('svm', SVC())])\n",
    "\n",
    "# Set up the Grid\n",
    "params_pipe_rf = {'svm__kernel': ['linear', 'rbf', 'sigmoid'],\n",
    "                  'svm__C': 10 ** np.arange(-2.0, 2.0, 1.0),\n",
    "                  'skb__k': [20, 25, 35, 50]}\n",
    "\n",
    "# Grid Search\n",
    "gscv_pipe_rf = GridSearchCV(estimator = pipe_rf,\n",
    "                            param_grid = params_pipe_rf,\n",
    "                            scoring='f1_weighted',\n",
    "                            cv=3,\n",
    "                            verbose=1)\n",
    "\n",
    "# Fit\n",
    "gscv_pipe_rf.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'skb__k': 50, 'svm__C': 10.0, 'svm__kernel': 'rbf'}"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "gscv_pipe_rf.best_params_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[mean: 0.92834, std: 0.00827, params: {'svm__C': 0.01, 'svm__kernel': 'linear', 'skb__k': 20},\n",
       " mean: 0.02049, std: 0.00020, params: {'svm__C': 0.01, 'svm__kernel': 'rbf', 'skb__k': 20},\n",
       " mean: 0.02049, std: 0.00020, params: {'svm__C': 0.01, 'svm__kernel': 'sigmoid', 'skb__k': 20},\n",
       " mean: 0.93210, std: 0.00814, params: {'svm__C': 0.10000000000000001, 'svm__kernel': 'linear', 'skb__k': 20},\n",
       " mean: 0.91718, std: 0.01364, params: {'svm__C': 0.10000000000000001, 'svm__kernel': 'rbf', 'skb__k': 20},\n",
       " mean: 0.02049, std: 0.00020, params: {'svm__C': 0.10000000000000001, 'svm__kernel': 'sigmoid', 'skb__k': 20},\n",
       " mean: 0.92667, std: 0.00805, params: {'svm__C': 1.0, 'svm__kernel': 'linear', 'skb__k': 20},\n",
       " mean: 0.95328, std: 0.00705, params: {'svm__C': 1.0, 'svm__kernel': 'rbf', 'skb__k': 20},\n",
       " mean: 0.02049, std: 0.00020, params: {'svm__C': 1.0, 'svm__kernel': 'sigmoid', 'skb__k': 20},\n",
       " mean: 0.90910, std: 0.01684, params: {'svm__C': 10.0, 'svm__kernel': 'linear', 'skb__k': 20}]"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "gscv_pipe_rf.grid_scores_[:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.98539568111664155"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "gscv_pipe_rf.best_score_"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Predict using the best model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "y_pred = gscv_pipe_rf.predict(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.98611111111111116"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.metrics import confusion_matrix, classification_report, accuracy_score\n",
    "\n",
    "accuracy_score(y_test, y_pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[39,  0,  0,  0,  0,  0,  0,  0,  0,  0],\n",
       "       [ 0, 29,  0,  0,  0,  0,  0,  0,  0,  0],\n",
       "       [ 0,  0, 37,  0,  0,  0,  0,  0,  0,  0],\n",
       "       [ 0,  0,  0, 34,  0,  1,  0,  0,  0,  0],\n",
       "       [ 0,  0,  0,  0, 40,  0,  0,  0,  0,  0],\n",
       "       [ 0,  0,  0,  0,  0, 40,  1,  0,  0,  0],\n",
       "       [ 0,  0,  0,  0,  0,  0, 41,  0,  0,  0],\n",
       "       [ 0,  0,  0,  0,  0,  0,  0, 31,  0,  0],\n",
       "       [ 0,  1,  0,  0,  0,  0,  0,  0, 35,  0],\n",
       "       [ 0,  0,  0,  0,  0,  1,  0,  1,  0, 29]])"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "confusion_matrix(y_test, gscv_pipe_rf.best_estimator_.predict(X_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "             precision    recall  f1-score   support\n",
      "\n",
      "          0       1.00      1.00      1.00        39\n",
      "          1       0.97      1.00      0.98        29\n",
      "          2       1.00      1.00      1.00        37\n",
      "          3       1.00      0.97      0.99        35\n",
      "          4       1.00      1.00      1.00        40\n",
      "          5       0.95      0.98      0.96        41\n",
      "          6       0.98      1.00      0.99        41\n",
      "          7       0.97      1.00      0.98        31\n",
      "          8       1.00      0.97      0.99        36\n",
      "          9       1.00      0.94      0.97        31\n",
      "\n",
      "avg / total       0.99      0.99      0.99       360\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print classification_report(y_test, gscv_pipe_rf.best_estimator_.predict(X_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Index([u'parameters', u'mean_validation_score', u'cv_validation_scores'], dtype='object')"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "df_1 = pd.DataFrame(gscv_pipe_rf.grid_scores_)\n",
    "df_1.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'skb__k': 50, 'svm__C': 10.0, 'svm__kernel': 'rbf'},\n",
       " {'skb__k': 35, 'svm__C': 10.0, 'svm__kernel': 'rbf'},\n",
       " {'skb__k': 50, 'svm__C': 1.0, 'svm__kernel': 'rbf'},\n",
       " {'skb__k': 35, 'svm__C': 1.0, 'svm__kernel': 'rbf'},\n",
       " {'skb__k': 50, 'svm__C': 0.10000000000000001, 'svm__kernel': 'linear'},\n",
       " {'skb__k': 50, 'svm__C': 10.0, 'svm__kernel': 'linear'},\n",
       " {'skb__k': 50, 'svm__C': 1.0, 'svm__kernel': 'linear'},\n",
       " {'skb__k': 50, 'svm__C': 0.01, 'svm__kernel': 'linear'},\n",
       " {'skb__k': 25, 'svm__C': 1.0, 'svm__kernel': 'rbf'},\n",
       " {'skb__k': 25, 'svm__C': 10.0, 'svm__kernel': 'rbf'}]"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_1.sort_values('mean_validation_score', ascending=False)[:10].loc[:, 'parameters'].tolist()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Explore the Grid Scores to choose a different model for production\n",
    "\n",
    "- Tradeoff Complexity vs. Generalizability"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "pd.concat([pd.DataFrame(df_1['parameters'].tolist()),\n",
    "           df_1['mean_validation_score']], axis=1).sort_values('mean_validation_score', ascending=False).iloc[:10, :]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## Pipeline For Text Mining"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "import csv\n",
    "import json\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import os\n",
    "from sklearn import cross_validation \n",
    "from sklearn import datasets\n",
    "from sklearn import decomposition\n",
    "from sklearn import ensemble\n",
    "from sklearn import feature_extraction\n",
    "from sklearn import feature_selection\n",
    "from sklearn import grid_search\n",
    "from sklearn import metrics\n",
    "from sklearn import naive_bayes\n",
    "from sklearn import pipeline\n",
    "from sklearn import tree\n",
    "\n",
    "import seaborn as sns\n",
    "\n",
    "pd.set_option('display.max_columns', None)\n",
    "\n",
    "_DATA_DIR ='data'\n",
    "_SPAM_DATA_PATH = os.path.join(_DATA_DIR, 'SMSSpamCollection')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### [Dataset Explanation](https://archive.ics.uci.edu/ml/datasets/SMS+Spam+Collection)\n",
    "- A collection of 425 SMS spam messages was manually extracted from the Grumbletext Web site. This is a UK forum in which cell phone users make public claims about SMS spam messages, most of them without reporting the very spam message received. The identification of the text of spam messages in the claims is a very hard and time-consuming task, and it involved carefully scanning hundreds of web pages. The Grumbletext Web site is: [Web Link]. \n",
    "<br><br>\n",
    "- A subset of 3,375 SMS randomly chosen ham messages of the NUS SMS Corpus (NSC), which is a dataset of about 10,000 legitimate messages collected for research at the Department of Computer Science at the National University of Singapore. The messages largely originate from Singaporeans and mostly from students attending the University. These messages were collected from volunteers who were made aware that their contributions were going to be made publicly available. The NUS SMS Corpus is avalaible at: [Web Link]. <br><br>\n",
    "- A list of 450 SMS ham messages collected from Caroline Tag's PhD Thesis available at [Web Link]. <br><br>\n",
    "- Finally, we have incorporated the SMS Spam Corpus v.0.1 Big. It has 1,002 SMS ham messages and 322 spam messages and it is public available at: [Web Link]. <br><br>\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "df = pd.read_csv('./smsspamcollection/SMSSpamCollection', sep='\\t', header=None, names=['Label', 'Text'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Label</th>\n",
       "      <th>Text</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>ham</td>\n",
       "      <td>Go until jurong point, crazy.. Available only ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>ham</td>\n",
       "      <td>Ok lar... Joking wif u oni...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>spam</td>\n",
       "      <td>Free entry in 2 a wkly comp to win FA Cup fina...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>ham</td>\n",
       "      <td>U dun say so early hor... U c already then say...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>ham</td>\n",
       "      <td>Nah I don't think he goes to usf, he lives aro...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  Label                                               Text\n",
       "0   ham  Go until jurong point, crazy.. Available only ...\n",
       "1   ham                      Ok lar... Joking wif u oni...\n",
       "2  spam  Free entry in 2 a wkly comp to win FA Cup fina...\n",
       "3   ham  U dun say so early hor... U c already then say...\n",
       "4   ham  Nah I don't think he goes to usf, he lives aro..."
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Nothing very fancy. Let's convert the pandas dataframe into a numpy matrix, and then do a cross validation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "y = (df.Label == 'ham').values.astype(int)\n",
    "X = df.Text.values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from sklearn.cross_validation import train_test_split\n",
    "X_train, X_test, y_train, y_test = train_test_split(X,\n",
    "                                                    y,\n",
    "                                                    test_size=0.2,\n",
    "                                                    random_state=0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Let's Create our pipeline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "`CountVectorizer` helps you find a word count for each word in a document.\n",
    "This converts a document into a Pandas Series with the word as the index and the count as the value.\n",
    "Iterating over all documents (for ex, 300 sms's) we get many such Series that are then assembled into a dataFrame.\n",
    "That's how we convert a column of text into a dataframe with numbers on which we can perform machine learning\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "sent1 = \"Sachin Tendulkar scored over ten thousand runs in limited overs cricket.\"\n",
    "sent2 = \"Cricket and football are the most watched sports in the world\"\n",
    "sent3 = \"The mitochondria is the powerhouse of the cell.\"\n",
    "sent4 = \"Inside the nucleus of a cell, we find the DNA for any animal.\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from pandas import Series, DataFrame"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def counter(sent):\n",
    "    return Series({word:sent.count(word) for word in sent.split(' ')})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Sachin       1\n",
       "Tendulkar    1\n",
       "cricket.     1\n",
       "in           2\n",
       "limited      1\n",
       "over         2\n",
       "overs        1\n",
       "runs         1\n",
       "scored       1\n",
       "ten          1\n",
       "thousand     1\n",
       "dtype: int64"
      ]
     },
     "execution_count": 52,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "counter(sent1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Cricket     1\n",
       "and         1\n",
       "are         1\n",
       "football    1\n",
       "in          1\n",
       "most        1\n",
       "sports      1\n",
       "the         2\n",
       "watched     1\n",
       "world       1\n",
       "dtype: int64"
      ]
     },
     "execution_count": 53,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "counter(sent2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> Each word will become a column in a DataFrame and the rows will be counts of that word in each document."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Let's do this on the real data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import CountVectorizer, TfidfTransformer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from sklearn.naive_bayes import BernoulliNB\n",
    "\n",
    "pipe = Pipeline(steps=[('vect', CountVectorizer()),\n",
    "                       ('tfidf', TfidfTransformer()),\n",
    "                       ('bernoulli', BernoulliNB())])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Pipeline(steps=[('vect', CountVectorizer(analyzer=u'word', binary=False, decode_error=u'strict',\n",
       "        dtype=<type 'numpy.int64'>, encoding=u'utf-8', input=u'content',\n",
       "        lowercase=True, max_df=1.0, max_features=None, min_df=1,\n",
       "        ngram_range=(1, 1), preprocessor=None, stop_words=None,\n",
       "        st...e_idf=True)), ('bernoulli', BernoulliNB(alpha=1.0, binarize=0.0, class_prior=None, fit_prior=True))])"
      ]
     },
     "execution_count": 78,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pipe.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Functions that are applicable to estimators are also applicable to Pipelines. That is one of the most powerful premise of the pipeline after all. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.98116591928251118"
      ]
     },
     "execution_count": 79,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "accuracy_score(pipe.predict(X_test), y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "By now, we know that this score is not very meaningful, let's look at the confusion matrix!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Populating the interactive namespace from numpy and matplotlib\n"
     ]
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAecAAAFhCAYAAABK5GKRAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x1107277d0>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import seaborn as sns\n",
    "%pylab inline\n",
    "sns.heatmap(confusion_matrix(pipe.predict(X_test), y_test), annot=True,  fmt='');"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "             precision    recall  f1-score   support\n",
      "\n",
      "          0       0.87      1.00      0.93       139\n",
      "          1       1.00      0.98      0.99       976\n",
      "\n",
      "avg / total       0.98      0.98      0.98      1115\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(classification_report(pipe.predict(X_test), y_test))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This is actually pretty good. We classify all of the spam as spam whereas couple of normal messages get into spam folder. Not ideal but pretty good for our first try. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Upto here, you may start convincing yourself how pipeline would be much better and useful and easier than applying each separate component in the machine learning pipeline. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Grid Search in Pipeline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "One might apply grid search to the pipeline similar to what we did on the estimator as well. Then one may ask, how do we pass parameters for vectorizer, feature selector and classifier. In the grid search of an estimator, this would be easy as you could pass a `dictionary` which has the keys for the parameters and the parameters as lists that you want to optimize. However, the things in `pipeline` is not that straightforward. First, what if two estimators share the same parameter name and you want to give different values in the search space. What if you do not want to pass any list of parameter to one and pass to the other one? In order to handle this ambiguity, `pipeline` accepts parameters in the form of `{name}__{parameter}` in the dictionary where the `{name}` is the name of the step that you are passing to the pipeline and the parameter is the parameter name that you want to optimize in that step."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h4> Why double leading underscore?</h4>\n",
    "> <b>__double_leading_underscore</b>: when naming a class attribute, invokes name mangling (inside class FooBar, __boo becomes _FooBar__boo; ) \n",
    "\n",
    "From [PEP 8](http://legacy.python.org/dev/peps/pep-0008/)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the pipeline, we have only two steps: `pca` and `gbf`. There will be two different parameter family in the parameters dictionary; the ones that start with `vectorizer` and the ones start with `gbf`, both of the parameter names will be followed by `__`(double underscore) and then the original parameter name. For an example, if I want to pass `n_estimators` as a parameter to `GradientBoostingClassifier` which was named `gbf` in the pipeline, I need to pass as `gbf_n_estimators`. Since I will be looking at different values for each parameter, I will pass a list in the values of that dictionary."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "CountVectorizer?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "TfidfTransformer?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "BernoulliNB?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "params = dict(vect__max_df=[0.5, 1.0],\n",
    "              vect__max_features=[None, 10000, 200000],\n",
    "              vect__ngram_range=[(1, 1), (1, 2)],\n",
    "              tfidf__use_idf=[True, False],\n",
    "              tfidf__norm=['l1', 'l2'],\n",
    "              bernoulli__alpha=[0, .5, 1],\n",
    "              bernoulli__binarize=[None, .1, .5],\n",
    "              bernoulli__fit_prior=[True, False]\n",
    "             )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "After preparing the parameters, we are ready to pass this parameter and the pipeline to the `RandomizedSearchCV`."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Wait, what? I thought we are using `GridSearchCV` like we did earlier notebook. Oh, that. Instead of searching __all__ the parameters in the parameter space, `RandomizedSearchCV` makes a randomized search. The total number of parameters that have been tried for optimizing the search parameters is determined by an optional parameter `n_iter`. If you set the `n_iter` to be 20, then 20 different total number of parameter combinations will be tried out in optimizing the parameter search space. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from sklearn.grid_search import RandomizedSearchCV\n",
    "RandomizedSearchCV?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "n_iter_search = 100\n",
    "random_search = RandomizedSearchCV(pipe, \n",
    "                                   param_distributions=params,\n",
    "                                   n_iter=n_iter_search)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/lr/anaconda2/lib/python2.7/site-packages/sklearn/naive_bayes.py:766: RuntimeWarning: divide by zero encountered in log\n",
      "  self.feature_log_prob_ = (np.log(smoothed_fc)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "RandomizedSearchCV(cv=None, error_score='raise',\n",
       "          estimator=Pipeline(steps=[('vect', CountVectorizer(analyzer=u'word', binary=False, decode_error=u'strict',\n",
       "        dtype=<type 'numpy.int64'>, encoding=u'utf-8', input=u'content',\n",
       "        lowercase=True, max_df=1.0, max_features=None, min_df=1,\n",
       "        ngram_range=(1, 1), preprocessor=None, stop_words=None,\n",
       "        st...e_idf=True)), ('bernoulli', BernoulliNB(alpha=1.0, binarize=0.0, class_prior=None, fit_prior=True))]),\n",
       "          fit_params={}, iid=True, n_iter=100, n_jobs=1,\n",
       "          param_distributions={'vect__ngram_range': [(1, 1), (1, 2)], 'vect__max_df': [0.5, 1.0], 'tfidf__use_idf': [True, False], 'tfidf__norm': ['l1', 'l2'], 'bernoulli__binarize': [None, 0.1, 0.5], 'bernoulli__fit_prior': [True, False], 'bernoulli__alpha': [0, 0.5, 1], 'vect__max_features': [None, 10000, 200000]},\n",
       "          pre_dispatch='2*n_jobs', random_state=None, refit=True,\n",
       "          scoring=None, verbose=0)"
      ]
     },
     "execution_count": 89,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "random_search.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'bernoulli__alpha': 0.5,\n",
       " 'bernoulli__binarize': 0.1,\n",
       " 'bernoulli__fit_prior': False,\n",
       " 'tfidf__norm': 'l2',\n",
       " 'tfidf__use_idf': True,\n",
       " 'vect__max_df': 0.5,\n",
       " 'vect__max_features': None,\n",
       " 'vect__ngram_range': (1, 1)}"
      ]
     },
     "execution_count": 90,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "random_search.best_params_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Pipeline(steps=[('vect', CountVectorizer(analyzer=u'word', binary=False, decode_error=u'strict',\n",
       "        dtype=<type 'numpy.int64'>, encoding=u'utf-8', input=u'content',\n",
       "        lowercase=True, max_df=0.5, max_features=None, min_df=1,\n",
       "        ngram_range=(1, 1), preprocessor=None, stop_words=None,\n",
       "        st..._idf=True)), ('bernoulli', BernoulliNB(alpha=0.5, binarize=0.1, class_prior=None, fit_prior=False))])"
      ]
     },
     "execution_count": 91,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "random_search.best_estimator_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.9838456360780794"
      ]
     },
     "execution_count": 92,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "random_search.best_score_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>parameters</th>\n",
       "      <th>mean_validation_score</th>\n",
       "      <th>cv_validation_scores</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>68</th>\n",
       "      <td>{u'vect__ngram_range': (1, 1), u'vect__max_df'...</td>\n",
       "      <td>0.983846</td>\n",
       "      <td>[0.985195154778, 0.981830417227, 0.984511784512]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>56</th>\n",
       "      <td>{u'vect__ngram_range': (1, 1), u'vect__max_df'...</td>\n",
       "      <td>0.983846</td>\n",
       "      <td>[0.985195154778, 0.981830417227, 0.984511784512]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>{u'vect__ngram_range': (1, 1), u'vect__max_df'...</td>\n",
       "      <td>0.982051</td>\n",
       "      <td>[0.983176312248, 0.979138627187, 0.983838383838]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>34</th>\n",
       "      <td>{u'vect__ngram_range': (1, 1), u'vect__max_df'...</td>\n",
       "      <td>0.972403</td>\n",
       "      <td>[0.971063257066, 0.970390309556, 0.975757575758]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>30</th>\n",
       "      <td>{u'vect__ngram_range': (1, 1), u'vect__max_df'...</td>\n",
       "      <td>0.972403</td>\n",
       "      <td>[0.971063257066, 0.970390309556, 0.975757575758]</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                           parameters  mean_validation_score  \\\n",
       "68  {u'vect__ngram_range': (1, 1), u'vect__max_df'...               0.983846   \n",
       "56  {u'vect__ngram_range': (1, 1), u'vect__max_df'...               0.983846   \n",
       "5   {u'vect__ngram_range': (1, 1), u'vect__max_df'...               0.982051   \n",
       "34  {u'vect__ngram_range': (1, 1), u'vect__max_df'...               0.972403   \n",
       "30  {u'vect__ngram_range': (1, 1), u'vect__max_df'...               0.972403   \n",
       "\n",
       "                                cv_validation_scores  \n",
       "68  [0.985195154778, 0.981830417227, 0.984511784512]  \n",
       "56  [0.985195154778, 0.981830417227, 0.984511784512]  \n",
       "5   [0.983176312248, 0.979138627187, 0.983838383838]  \n",
       "34  [0.971063257066, 0.970390309556, 0.975757575758]  \n",
       "30  [0.971063257066, 0.970390309556, 0.975757575758]  "
      ]
     },
     "execution_count": 93,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "DataFrame(random_search.grid_scores_).sort_values('mean_validation_score', ascending=False)[:5]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "But we know that the classification accuracy is not the best metric to optimize as we already have a quite high classification accuracy. Let's optimize for `f1` score by passing optional `scoring='f1'`!\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "random_search = RandomizedSearchCV(pipe, \n",
    "                                   param_distributions=params,\n",
    "                                   n_iter=25, \n",
    "                                   scoring='f1_weighted')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "RandomizedSearchCV(cv=None, error_score='raise',\n",
       "          estimator=Pipeline(steps=[('vect', CountVectorizer(analyzer=u'word', binary=False, decode_error=u'strict',\n",
       "        dtype=<type 'numpy.int64'>, encoding=u'utf-8', input=u'content',\n",
       "        lowercase=True, max_df=1.0, max_features=None, min_df=1,\n",
       "        ngram_range=(1, 1), preprocessor=None, stop_words=None,\n",
       "        st...e_idf=True)), ('bernoulli', BernoulliNB(alpha=1.0, binarize=0.0, class_prior=None, fit_prior=True))]),\n",
       "          fit_params={}, iid=True, n_iter=25, n_jobs=1,\n",
       "          param_distributions={'vect__ngram_range': [(1, 1), (1, 2)], 'vect__max_df': [0.5, 1.0], 'tfidf__use_idf': [True, False], 'tfidf__norm': ['l1', 'l2'], 'bernoulli__binarize': [None, 0.1, 0.5], 'bernoulli__fit_prior': [True, False], 'bernoulli__alpha': [0, 0.5, 1], 'vect__max_features': [None, 10000, 200000]},\n",
       "          pre_dispatch='2*n_jobs', random_state=None, refit=True,\n",
       "          scoring='f1_weighted', verbose=0)"
      ]
     },
     "execution_count": 95,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "random_search.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'bernoulli__alpha': 0.5,\n",
       " 'bernoulli__binarize': 0.1,\n",
       " 'bernoulli__fit_prior': False,\n",
       " 'tfidf__norm': 'l2',\n",
       " 'tfidf__use_idf': True,\n",
       " 'vect__max_df': 1.0,\n",
       " 'vect__max_features': None,\n",
       " 'vect__ngram_range': (1, 1)}"
      ]
     },
     "execution_count": 96,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "random_search.best_params_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Pipeline(steps=[('vect', CountVectorizer(analyzer=u'word', binary=False, decode_error=u'strict',\n",
       "        dtype=<type 'numpy.int64'>, encoding=u'utf-8', input=u'content',\n",
       "        lowercase=True, max_df=1.0, max_features=None, min_df=1,\n",
       "        ngram_range=(1, 1), preprocessor=None, stop_words=None,\n",
       "        st..._idf=True)), ('bernoulli', BernoulliNB(alpha=0.5, binarize=0.1, class_prior=None, fit_prior=False))])"
      ]
     },
     "execution_count": 97,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "random_search.best_estimator_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>parameters</th>\n",
       "      <th>mean_validation_score</th>\n",
       "      <th>cv_validation_scores</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>{u'vect__ngram_range': (1, 1), u'vect__max_df'...</td>\n",
       "      <td>0.983449</td>\n",
       "      <td>[0.98481759847, 0.981434067459, 0.984094885573]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>{u'vect__ngram_range': (1, 1), u'vect__max_df'...</td>\n",
       "      <td>0.981590</td>\n",
       "      <td>[0.982726323479, 0.978580641114, 0.983463743464]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>{u'vect__ngram_range': (1, 1), u'vect__max_df'...</td>\n",
       "      <td>0.973302</td>\n",
       "      <td>[0.972500664248, 0.971978934306, 0.975426850848]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>{u'vect__ngram_range': (1, 1), u'vect__max_df'...</td>\n",
       "      <td>0.973054</td>\n",
       "      <td>[0.971758153593, 0.971978934306, 0.975426850848]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>{u'vect__ngram_range': (1, 2), u'vect__max_df'...</td>\n",
       "      <td>0.948235</td>\n",
       "      <td>[0.948663306993, 0.951869696613, 0.944168416069]</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                           parameters  mean_validation_score  \\\n",
       "2   {u'vect__ngram_range': (1, 1), u'vect__max_df'...               0.983449   \n",
       "16  {u'vect__ngram_range': (1, 1), u'vect__max_df'...               0.981590   \n",
       "11  {u'vect__ngram_range': (1, 1), u'vect__max_df'...               0.973302   \n",
       "15  {u'vect__ngram_range': (1, 1), u'vect__max_df'...               0.973054   \n",
       "14  {u'vect__ngram_range': (1, 2), u'vect__max_df'...               0.948235   \n",
       "\n",
       "                                cv_validation_scores  \n",
       "2    [0.98481759847, 0.981434067459, 0.984094885573]  \n",
       "16  [0.982726323479, 0.978580641114, 0.983463743464]  \n",
       "11  [0.972500664248, 0.971978934306, 0.975426850848]  \n",
       "15  [0.971758153593, 0.971978934306, 0.975426850848]  \n",
       "14  [0.948663306993, 0.951869696613, 0.944168416069]  "
      ]
     },
     "execution_count": 98,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "DataFrame(random_search.grid_scores_).sort_values('mean_validation_score', ascending=False)[:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "best_pipe = Pipeline([('vect', CountVectorizer(ngram_range=(1, 1), max_df=1.0, max_features=20000)),\n",
    "                      ('tfidf', TfidfTransformer(use_idf=True, norm='l2')),\n",
    "                      (\"bernoulli\", BernoulliNB(binarize=0.1, alpha=.5, fit_prior=True))])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Pipeline(steps=[('vect', CountVectorizer(analyzer=u'word', binary=False, decode_error=u'strict',\n",
       "        dtype=<type 'numpy.int64'>, encoding=u'utf-8', input=u'content',\n",
       "        lowercase=True, max_df=1.0, max_features=20000, min_df=1,\n",
       "        ngram_range=(1, 1), preprocessor=None, stop_words=None,\n",
       "        s...e_idf=True)), ('bernoulli', BernoulliNB(alpha=0.5, binarize=0.1, class_prior=None, fit_prior=True))])"
      ]
     },
     "execution_count": 100,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "best_pipe.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAecAAAFhCAYAAABK5GKRAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nOEuSlDGGsyRJGWM4S5KUMf8flSxZmjwE80EAAAAASUVORK5CYII=\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x111a62fd0>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "sns.heatmap(metrics.confusion_matrix(best_pipe.predict(X_test), y_test), annot=True,  fmt='');"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "             precision    recall  f1-score   support\n",
      "\n",
      "          0       0.89      0.99      0.94       144\n",
      "          1       1.00      0.98      0.99       971\n",
      "\n",
      "avg / total       0.99      0.98      0.98      1115\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(metrics.classification_report(best_pipe.predict(X_test), y_test))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This is a small improvement for humanity and also for us sadly :(. But we could do better with a different classifier, with more parameters,\n",
    "and even using grid search instead of randomized! There are paramters that wait for us to optimize!"
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python [default]",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.12"
  },
  "widgets": {
   "state": {},
   "version": "1.1.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
