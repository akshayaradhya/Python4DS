{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Installing Spark\n",
    "\n",
    "---\n",
    "\n",
    "This file has instructions to \n",
    "\n",
    "* install PySpark **locally** on your own computer, and \n",
    "* to integrate with the Jupyter Notebook.\n",
    "\n",
    "---\n",
    "## 0. Check and install dependencies\n",
    "\n",
    "- Is **`Homebrew`** installed?\n",
    "    - Get it [here](http://brew.sh/)\n",
    "    \n",
    "\n",
    "- Is **`Java`** Installed? (Run `java -version` to check)\n",
    "    - Install (by downloading the `.dmg` files) \n",
    "        - **Java Software Development Kit** [SDK 8](http://www.oracle.com/technetwork/java/javase/downloads/jdk8-downloads-2133151.html) and \n",
    "        - **Java Runtime Environment** [JRE 8](http://www.oracle.com/technetwork/java/javase/downloads/jre8-downloads-2133155.html) \n",
    "        \n",
    "\n",
    "- Is **`Scala`** installed? (Run `scala -version` to check)\n",
    "    - Run \n",
    "        - `brew install scala`  \n",
    "        - `brew install sbt`\n",
    "\n",
    "---\n",
    "\n",
    "## 1. Two Methods of Installing Spark\n",
    "\n",
    "\n",
    "## $Build$ It \n",
    "\n",
    "- Download the latest version from the official Spark [downloads](http://spark.apache.org/downloads.html) page as a `.tar` file <br><br>\n",
    "- Move the `.tar` file to your preferred installation directory <br><br>\n",
    "- Untar it by running (change version number!)\n",
    "    - `tar -xvzf spark-1.5.1-bin-hadoop2.4.tar` <br><br>\n",
    "- `cd` to the untarred folder and build Spark using the Scala Build Tools by running the following command. (This will take a while.)\n",
    "    - `sbt assembly` \n",
    "\n",
    "\n",
    "## $Brew$ It \n",
    "\n",
    "- If all dependencies are met, simply open the Terminal and run the following\n",
    "    - `brew install apache-spark`\n",
    "---\n",
    "\n",
    "## 2. Using Spark with Jupyter Notebooks\n",
    "\n",
    "\n",
    "- To make Spark run with Jupyter Notebooks by default, we have to pass some parameters to the system `PATH`<br><br>\n",
    "- Open up the Terminal and type out the following commands\n",
    "\n",
    "\n",
    "        export PYSPARK_DRIVER_PYTHON=ipython\n",
    "        export PYSPARK_DRIVER_PYTHON_OPTS=\"notebook --no-browser --port=7777\"\n",
    "\n",
    "- Then just run `$ pyspark` on a new line of the Terminal <br><br>\n",
    "- Navigate to [http://localhost:7777/tree](http://localhost:7777/) and launch a new notebook.<br><br>\n",
    "- Check if everything worked by running `sc` in a cell. If the PySpark Context exists, we're ready to use Spark!<br><br>\n",
    "- Run the following code to create an RDD\n",
    "\n",
    "        x = sc.parallelize(range(10))\n",
    "        print type(x)\n",
    " \n",
    "--- \n",
    " \n",
    "## 3. Examples\n",
    "\n",
    "- [Word Count](http://nbviewer.jupyter.org/github/marek5050/Hadoop_Examples/blob/master/SparkieNET.ipynb). Because every first example in the Big Data world has to be.\n",
    "- Data on S3. Spark on a Cluster. 1TB Reddit comments data. [Link](http://blog.insightdatalabs.com/jupyter-on-apache-spark-step-by-step/)\n",
    "\n",
    "---\n",
    "\n",
    "## 4. Further Reading\n",
    "\n",
    "- StackOverflow Top Questions\n",
    "    - [Spark](http://stackoverflow.com/questions/tagged/apache-spark)\n",
    "    - [PySpark](http://stackoverflow.com/questions/tagged/pyspark)\n",
    "    - [Spark SQL](http://stackoverflow.com/questions/tagged/apache-spark-sql)\n",
    "    - [DataFrame](http://stackoverflow.com/questions/tagged/spark-dataframe)\n",
    "    - [MLLIB](http://stackoverflow.com/questions/tagged/apache-spark-mllib) \n",
    "    \n",
    "- [Official Documentation](http://spark.apache.org/docs/latest/) Version 1.5.0\n",
    "- [Spark Packages](http://spark-packages.org/)\n",
    "- Understand the [difference between MR and Spark](http://blog.cloudera.com/blog/2014/09/how-to-translate-from-mapreduce-to-apache-spark/)\n",
    "\n",
    "\n",
    "## 5. Tutorial Videos\n",
    "\n",
    "- [Sparkling Pandas](https://www.youtube.com/watch?v=AcyI_V8FeIU) with Holden Karau and Juliet Hougland at PyGotham 2014\n",
    "- [Introduction to Spark with Python](https://www.youtube.com/watch?v=9xYfNznjClE) with Orlando Karam @PyCon2015\n",
    "- [Practical Machine Learning Pipelines with MLlib](https://www.youtube.com/watch?v=Riuee7qxdX4) with Joseph Bradley (Databricks)\n",
    "- *Spark Summit 2015 Videos*\n",
    "\t- [Keynotes](https://www.youtube.com/playlist?list=PL-x35fyliRwgdKsaLFMwl-Q-vSd7-X6mi)\n",
    "\t- Tutorials: [Intro to Apache Spark & Advanced Spark](https://www.youtube.com/playlist?list=PL-x35fyliRwioDix9XjD3HptH8ro55SuB)\n",
    "\t- Data Science Track: [Tutorials](https://www.youtube.com/playlist?list=PL-x35fyliRwhP52fwDqULJLOnqnrN5nDs)\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
