{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* [DataFrame API](http://nbviewer.ipython.org/github/jkthompson/pyspark-pictures/blob/master/pyspark-pictures-dataframes.ipynb)\n",
    "* [GitHub](https://github.com/jkthompson/pyspark-pictures)\n",
    "* [related blog post](http://data-frack.blogspot.com/2015/01/visual-mnemonics-for-pyspark-api.html)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img align=left src=\"files/images/pyspark-page2.svg\" width=500 height=500 />"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Click on a picture to view pyspark docs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import IPython\n",
    "print(\"pyspark version:\" + str(sc.version))\n",
    "print(\"Ipython version:\" + str(IPython.__version__))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<pyspark.context.SparkContext at 0x7f7f8afc1dd0>"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sc"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a href=\"http://spark.apache.org/docs/1.2.0/api/python/pyspark.html#pyspark.RDD.map\">\n",
    "<img align=left src=\"files/images/pyspark-page3.svg\" width=500 height=500 />\n",
    "</a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1, 2, 3]\n",
      "[(1, 1), (2, 4), (3, 9)]\n"
     ]
    }
   ],
   "source": [
    "# map\n",
    "x = sc.parallelize([1, 2, 3]) \n",
    "# sc = spark context, parallelize creates an RDD from the passed object\n",
    "\n",
    "y = x.map(lambda x: (x, x**2))\n",
    "\n",
    "print(x.collect())  # collect copies RDD elements to a list on the driver\n",
    "print(y.collect())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "4"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x.getNumPartitions()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a href=\"http://spark.apache.org/docs/1.2.0/api/python/pyspark.html#pyspark.RDD.flatMap\">\n",
    "<img align=left src=\"files/images/pyspark-page4.svg\" width=500 height=500 />\n",
    "</a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# flatMap\n",
    "x = sc.parallelize([1,2,3])\n",
    "y = x.flatMap(lambda x: (x, 100*x, x**2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[1, 2, 3]"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(1, 100, 1), (2, 200, 4), (3, 300, 9)]"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x.map(lambda x: (x, 100*x, x**2)).collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[1, 100, 1, 2, 200, 4, 3, 300, 9]"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y.collect()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a href=\"http://spark.apache.org/docs/1.2.0/api/python/pyspark.html#pyspark.RDD.getNumPartitions\">\n",
    "<img align=left src=\"files/images/pyspark-page7.svg\" width=500 height=500 />\n",
    "</a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[0, 1, 2, 3], [4, 5, 6, 7], [8, 9, 10, 11], [12, 13, 14, 15], [16, 17, 18, 19]]\n",
      "5\n"
     ]
    }
   ],
   "source": [
    "# getNumPartitions\n",
    "x = sc.parallelize(range(20), 5)\n",
    "y = x.getNumPartitions()\n",
    "\n",
    "print(x.glom().collect())\n",
    "print(y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "x = sc.parallelize([[1,2,3], [4,5,6], [7,8,9], 12, 45], 3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "x.getNumPartitions()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[[0, 1, 2, 3],\n",
       " [4, 5, 6, 7],\n",
       " [8, 9, 10, 11],\n",
       " [12, 13, 14, 15],\n",
       " [16, 17, 18, 19]]"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x.glom().collect()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a href=\"http://spark.apache.org/docs/1.2.0/api/python/pyspark.html#pyspark.RDD.mapPartitions\">\n",
    "<img align=left src=\"files/images/pyspark-page5.svg\" width=500 height=500 />\n",
    "</a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "http://stackoverflow.com/questions/231767/what-does-the-yield-keyword-do"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[1], [2, 3]]\n",
      "[[1], [5]]\n"
     ]
    }
   ],
   "source": [
    "# mapPartitions\n",
    "x = sc.parallelize([1,2,3], 2)\n",
    "\n",
    "def f(iterator): \n",
    "    yield sum(iterator)\n",
    "\n",
    "y = x.mapPartitions(f)\n",
    "\n",
    "# glom() flattens elements on the same partition\n",
    "print(x.glom().collect())  \n",
    "print(y.glom().collect())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "rdd = sc.parallelize([1, 2, 3, 4, 5, 6, 7], 2)\n",
    "\n",
    "print rdd.glom().collect()\n",
    "print rdd.mapPartitions(f).collect()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a href=\"http://spark.apache.org/docs/1.2.0/api/python/pyspark.html#pyspark.RDD.mapPartitionsWithIndex\">\n",
    "<img align=left src=\"files/images/pyspark-page6.svg\" width=500 height=500 />\n",
    "</a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# mapPartitionsWithIndex\n",
    "x = sc.parallelize([1,2,3], 2)\n",
    "def f(partitionIndex, iterator): yield (partitionIndex,sum(iterator))\n",
    "y = x.mapPartitionsWithIndex(f)\n",
    "print(x.glom().collect())  # glom() flattens elements on the same partition\n",
    "print(y.glom().collect())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a href=\"http://spark.apache.org/docs/1.2.0/api/python/pyspark.html#pyspark.RDD.filter\">\n",
    "<img align=left src=\"files/images/pyspark-page8.svg\" width=500 height=500 />\n",
    "</a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# filter\n",
    "x = sc.parallelize([1,2,3])\n",
    "y = x.filter(lambda x: x%2 == 1)  # filters out even elements\n",
    "\n",
    "print(x.collect())\n",
    "print(y.collect())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a href=\"http://spark.apache.org/docs/1.2.0/api/python/pyspark.html#pyspark.RDD.distinct\">\n",
    "<img align=left src=\"files/images/pyspark-page9.svg\" width=500 height=500 />\n",
    "</a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# distinct\n",
    "x = sc.parallelize(list('aabbccddeessffhb'))\n",
    "y = x.distinct()\n",
    "\n",
    "print(x.collect())\n",
    "print(y.collect())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a href=\"http://spark.apache.org/docs/1.2.0/api/python/pyspark.html#pyspark.RDD.sample\">\n",
    "<img align=left src=\"files/images/pyspark-page10.svg\" width=500 height=500 />\n",
    "</a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# sample\n",
    "x = sc.parallelize(range(7))\n",
    "ylist = [x.sample(withReplacement=False, fraction=0.5) for i in range(5)] \n",
    "# call 'sample' 5 times\n",
    "print('x = ' + str(x.collect()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "ylist[0].collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "for cnt, y in enumerate(ylist):\n",
    "    print('sample:' + str(cnt) + ' y = ' +  str(y.collect()))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a href=\"http://spark.apache.org/docs/1.2.0/api/python/pyspark.html#pyspark.RDD.takeSample\">\n",
    "<img align=left src=\"files/images/pyspark-page11.svg\" width=500 height=500 />\n",
    "</a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# takeSample\n",
    "x = sc.parallelize(range(7))\n",
    "ylist = [x.takeSample(withReplacement=False, num=3) for i in range(5)]  \n",
    "# call 'sample' 5 times"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "print('x = ' + str(x.collect()))\n",
    "\n",
    "for cnt, y in enumerate(ylist):\n",
    "    print('sample:' + str(cnt) + ' y = ' +  str(y))  # no collect on y"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a href=\"http://spark.apache.org/docs/1.2.0/api/python/pyspark.html#pyspark.RDD.union\">\n",
    "<img align=left src=\"files/images/pyspark-page12.svg\" width=500 height=500 />\n",
    "</a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# union\n",
    "x = sc.parallelize(['A','A','B'])\n",
    "y = sc.parallelize(['D','C','A'])\n",
    "z = x.union(y)\n",
    "print(x.collect())\n",
    "print(y.collect())\n",
    "print(z.collect())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a href=\"http://spark.apache.org/docs/1.2.0/api/python/pyspark.html#pyspark.RDD.intersection\">\n",
    "<img align=left src=\"files/images/pyspark-page13.svg\" width=500 height=500 />\n",
    "</a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# intersection\n",
    "x = sc.parallelize(['A','A','B'])\n",
    "y = sc.parallelize(['A','C','D'])\n",
    "z = x.intersection(y)\n",
    "print(x.collect())\n",
    "print(y.collect())\n",
    "print(z.collect())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a href=\"http://spark.apache.org/docs/1.2.0/api/python/pyspark.html#pyspark.RDD.sortByKey\">\n",
    "<img align=left src=\"files/images/pyspark-page14.svg\" width=500 height=500 />\n",
    "</a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "sc.parallelize"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('B', 1), ('A', 2), ('C', 3)]\n",
      "[('A', 2), ('B', 1), ('C', 3)]\n"
     ]
    }
   ],
   "source": [
    "# sortByKey\n",
    "x = sc.parallelize([('B', 1), ('A', 2), ('C', 3)])\n",
    "y = x.sortByKey()\n",
    "print(x.collect())\n",
    "print(y.collect())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a href=\"http://spark.apache.org/docs/1.2.0/api/python/pyspark.html#pyspark.RDD.sortBy\">\n",
    "<img align=left src=\"files/images/pyspark-page15.svg\" width=500 height=500 />\n",
    "</a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# sortBy\n",
    "x = sc.parallelize(['Cat','Apple','Bat'])\n",
    "\n",
    "def keyGen(val): \n",
    "    return val[0]\n",
    "\n",
    "y = x.sortBy(keyGen)\n",
    "print(y.collect())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a href=\"http://spark.apache.org/docs/1.2.0/api/python/pyspark.html#pyspark.RDD.glom\">\n",
    "<img align=left src=\"files/images/pyspark-page16.svg\" width=500 height=500 />\n",
    "</a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "x.glom?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['C', 'B', 'A', 'D', 'e', 'f']\n",
      "[['C', 'B'], ['A', 'D'], ['e', 'f']]\n"
     ]
    }
   ],
   "source": [
    "# glom\n",
    "x = sc.parallelize(['C','B','A', 'D', 'e', 'f'], 3)\n",
    "y = x.glom()\n",
    "\n",
    "print(x.collect()) \n",
    "print(y.collect())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a href=\"http://spark.apache.org/docs/1.2.0/api/python/pyspark.html#pyspark.RDD.cartesian\">\n",
    "<img align=left src=\"files/images/pyspark-page17.svg\" width=500 height=500 />\n",
    "</a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# cartesian\n",
    "x = sc.parallelize(['A','B'])\n",
    "y = sc.parallelize(['C','D'])\n",
    "z = x.cartesian(y)\n",
    "print(x.collect())\n",
    "print(y.collect())\n",
    "print(z.collect())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a href=\"http://spark.apache.org/docs/1.2.0/api/python/pyspark.html#pyspark.RDD.groupBy\">\n",
    "<img align=left src=\"files/images/pyspark-page18.svg\" width=500 height=500 />\n",
    "<"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0, 1, 2, 3, 4, 5, 6, 7, 8, 9]\n"
     ]
    }
   ],
   "source": [
    "# groupBy\n",
    "x = sc.parallelize(xrange(10))\n",
    "y = x.groupBy(lambda x: 'Odd' if (x%2 == 1) else 'Even')\n",
    "\n",
    "print(x.collect())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('Even', <pyspark.resultiterable.ResultIterable at 0x111d7cbd0>),\n",
       " ('Odd', <pyspark.resultiterable.ResultIterable at 0x111d27a10>)]"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('Even', [0, 2, 4, 6, 8]), ('Odd', [1, 3, 5, 7, 9])]\n"
     ]
    }
   ],
   "source": [
    "print([(j[0], [i for i in j[1]]) for j in y.collect()]) # y is nested, this iterates through it"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a href=\"http://spark.apache.org/docs/1.2.0/api/python/pyspark.html#pyspark.RDD.pipe\">\n",
    "<img align=left src=\"files/images/pyspark-page19.svg\" width=500 height=500 />\n",
    "</a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['A', 'Ba', 'C', 'AD']\n",
      "[u'A', u'Ba', u'AD']\n"
     ]
    }
   ],
   "source": [
    "# pipe\n",
    "x = sc.parallelize(['A', 'Ba', 'C', 'AD'])\n",
    "y = x.pipe('grep -i \"A\"') # calls out to grep, may fail under Windows \n",
    "\n",
    "print(x.collect())\n",
    "print(y.collect())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a href=\"http://spark.apache.org/docs/1.2.0/api/python/pyspark.html#pyspark.RDD.foreach\">\n",
    "<img align=left src=\"files/images/pyspark-page20.svg\" width=500 height=500 />\n",
    "</a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3\r\n",
      "1\r\n",
      "2\r\n"
     ]
    }
   ],
   "source": [
    "!head foreachExample.txt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1, 2, 3]\n",
      "None\n",
      "3\n",
      "1\n",
      "2\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# foreach\n",
    "from __future__ import print_function\n",
    "\n",
    "x = sc.parallelize([1, 2, 3])\n",
    "\n",
    "def f(el):\n",
    "    '''side effect: append the current RDD elements to a file'''\n",
    "    f1 = open(\"./foreachExample.txt\", 'a+') \n",
    "    print(el, file=f1)\n",
    "\n",
    "open('./foreachExample.txt', 'w').close()  # first clear the file contents\n",
    "\n",
    "y = x.foreach(f) \n",
    "# writes into foreachExample.txt\n",
    "\n",
    "print(x.collect())\n",
    "\n",
    "print(y) \n",
    "# foreach returns 'None'\n",
    "\n",
    "# print the contents of foreachExample.txt\n",
    "with open(\"./foreachExample.txt\", \"r\") as foreachExample:\n",
    "    print (foreachExample.read())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a href=\"http://spark.apache.org/docs/1.2.0/api/python/pyspark.html#pyspark.RDD.foreachPartition\">\n",
    "<img align=left src=\"files/images/pyspark-page21.svg\" width=500 height=500 />\n",
    "</a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# foreachPartition\n",
    "from __future__ import print_function\n",
    "\n",
    "x = sc.parallelize([1,2,3], 5)\n",
    "\n",
    "def f(parition):\n",
    "    '''side effect: append the current RDD partition contents to a file'''\n",
    "    f1 = open(\"./foreachPartitionExample.txt\", 'a+') \n",
    "    print([el for el in parition], file=f1)\n",
    "\n",
    "open('./foreachPartitionExample.txt', 'w').close()  # first clear the file contents\n",
    "\n",
    "y = x.foreachPartition(f) # writes into foreachExample.txt\n",
    "\n",
    "print(x.glom().collect())\n",
    "print(y)  # foreach returns 'None'\n",
    "# print the contents of foreachExample.txt\n",
    "with open(\"./foreachPartitionExample.txt\", \"r\") as foreachExample:\n",
    "    print (foreachExample.read())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a href=\"http://spark.apache.org/docs/1.2.0/api/python/pyspark.html#pyspark.RDD.collect\">\n",
    "<img align=left src=\"files/images/pyspark-page22.svg\" width=500 height=500 />\n",
    "</a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# collect\n",
    "x = sc.parallelize([1,2,3])\n",
    "y = x.collect()\n",
    "print(x)  # distributed\n",
    "print(y)  # not distributed"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a href=\"http://spark.apache.org/docs/1.2.0/api/python/pyspark.html#pyspark.RDD.reduce\">\n",
    "<img align=left src=\"files/images/pyspark-page23.svg\" width=500 height=500 />\n",
    "</a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "reduce?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Base Python example\n",
    "print reduce(lambda x, y: x + y, range(10))\n",
    "\n",
    "# Pandas example of reduce\n",
    "# reduce(lambda df1, df2: pd.merge(df1, df2), [df1, df2, df3, df4, df5])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1, 2, 3, 4, 5]\n",
      "120\n"
     ]
    }
   ],
   "source": [
    "# reduce\n",
    "x = sc.parallelize([1,2,3, 4, 5])\n",
    "\n",
    "y = x.reduce(lambda a, b: a * b)  # computes a cumulative sum\n",
    "\n",
    "print(x.collect())\n",
    "print(y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a href=\"http://spark.apache.org/docs/1.2.0/api/python/pyspark.html#pyspark.RDD.fold\">\n",
    "<img align=left src=\"files/images/pyspark-page24.svg\" width=500 height=500 />\n",
    "</a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# fold\n",
    "x = sc.parallelize(xrange(10), 5)\n",
    "\n",
    "neutral_zero_value = 0  \n",
    "# 0 for sum, 1 for multiplication\n",
    "\n",
    "y = x.fold(neutral_zero_value, lambda obj, accumulated: accumulated + obj) \n",
    "# computes cumulative sum\n",
    "\n",
    "print(x.glom().collect())\n",
    "print(y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a href=\"http://spark.apache.org/docs/1.2.0/api/python/pyspark.html#pyspark.RDD.aggregate\">\n",
    "<img align=left src=\"files/images/pyspark-page25.svg\" width=500 height=500 />\n",
    "</a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# aggregate\n",
    "x = sc.parallelize(range(1, 10))\n",
    "neutral_zero_value = (0, 1) # sum: x+0 = x, product: 1*x = x\n",
    "\n",
    "\n",
    "print(x.glom().collect())\n",
    "# seqOp = (lambda x, y: ) \n",
    "# combOp = (lambda x, y: )\n",
    "\n",
    "def seqOp(x, y):\n",
    "    print('Seq')\n",
    "    print(x)\n",
    "    print(y)\n",
    "    print(x[0], y)\n",
    "    print(x[1], y)\n",
    "    return (x[0] + y, x[1] * y)\n",
    "\n",
    "def combOp(x,y): \n",
    "    print('combOp') \n",
    "    print(x[0], y[0], x[0] + y[0]) \n",
    "    print(x[1], y[1], x[1] * y[1]) \n",
    "    return((x[0] + y[0], x[1] * y[1]))\n",
    "            \n",
    "\n",
    "y = x.aggregate(neutral_zero_value, seqOp, combOp)  \n",
    "# computes (cumulative sum, cumulative product)\n",
    "\n",
    "print(x.collect())\n",
    "print(y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a href=\"http://spark.apache.org/docs/1.2.0/api/python/pyspark.html#pyspark.RDD.max\">\n",
    "<img align=left src=\"files/images/pyspark-page26.svg\" width=500 height=500 />\n",
    "</a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# max\n",
    "x = sc.parallelize([1,3,2])\n",
    "y = x.max()\n",
    "print(x.collect())\n",
    "print(y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a href=\"http://spark.apache.org/docs/1.2.0/api/python/pyspark.html#pyspark.RDD.min\">\n",
    "<img align=left src=\"files/images/pyspark-page27.svg\" width=500 height=500 />\n",
    "</a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# min\n",
    "x = sc.parallelize([1,3,2])\n",
    "y = x.min()\n",
    "print(x.collect())\n",
    "print(y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a href=\"http://spark.apache.org/docs/1.2.0/api/python/pyspark.html#pyspark.RDD.sum\">\n",
    "<img align=left src=\"files/images/pyspark-page28.svg\" width=500 height=500 />\n",
    "</a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# sum\n",
    "x = sc.parallelize([1,3,2])\n",
    "y = x.sum()\n",
    "print(x.collect())\n",
    "print(y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a href=\"http://spark.apache.org/docs/1.2.0/api/python/pyspark.html#pyspark.RDD.count\">\n",
    "<img align=left src=\"files/images/pyspark-page29.svg\" width=500 height=500 />\n",
    "</a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# count\n",
    "x = sc.parallelize([1,3,2])\n",
    "y = x.count()\n",
    "print(x.collect())\n",
    "print(y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a href=\"http://spark.apache.org/docs/1.2.0/api/python/pyspark.html#pyspark.RDD.histogram\">\n",
    "<img align=left src=\"files/images/pyspark-page30.svg\" width=500 height=500 />\n",
    "</a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# histogram (example #1)\n",
    "x = sc.parallelize([1,3,1,2,3])\n",
    "y = x.histogram(buckets = 2)\n",
    "print(x.collect())\n",
    "print(y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# histogram (example #2)\n",
    "x = sc.parallelize([1,3,1,2,3])\n",
    "y = x.histogram([0,0.5,1,1.5,2,2.5,3,3.5])\n",
    "print(x.collect())\n",
    "print(y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a href=\"http://spark.apache.org/docs/1.2.0/api/python/pyspark.html#pyspark.RDD.mean\">\n",
    "<img align=left src=\"files/images/pyspark-page31.svg\" width=500 height=500 />\n",
    "</a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1, 3, 2]\n",
      "2.0\n"
     ]
    }
   ],
   "source": [
    "# mean\n",
    "x = sc.parallelize([1,3,2])\n",
    "y = x.mean()\n",
    "print(x.collect())\n",
    "print(y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a href=\"http://spark.apache.org/docs/1.2.0/api/python/pyspark.html#pyspark.RDD.variance\">\n",
    "<img align=left src=\"files/images/pyspark-page32.svg\" width=500 height=500 />\n",
    "</a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# variance\n",
    "x = sc.parallelize([1,3,2])\n",
    "y = x.variance()  # divides by N\n",
    "print(x.collect())\n",
    "print(y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a href=\"http://spark.apache.org/docs/1.2.0/api/python/pyspark.html#pyspark.RDD.stdev\">\n",
    "<img align=left src=\"files/images/pyspark-page33.svg\" width=500 height=500 />\n",
    "</a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# stdev\n",
    "x = sc.parallelize([1,3,2])\n",
    "y = x.stdev()  # divides by N\n",
    "print(x.collect())\n",
    "print(y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a href=\"http://spark.apache.org/docs/1.2.0/api/python/pyspark.html#pyspark.RDD.sampleStdev\">\n",
    "<img align=left src=\"files/images/pyspark-page34.svg\" width=500 height=500 />\n",
    "</a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# sampleStdev\n",
    "x = sc.parallelize([1,3,2])\n",
    "y = x.sampleStdev() # divides by N-1\n",
    "print(x.collect())\n",
    "print(y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a href=\"http://spark.apache.org/docs/1.2.0/api/python/pyspark.html#pyspark.RDD.sampleVariance\">\n",
    "<img align=left src=\"files/images/pyspark-page35.svg\" width=500 height=500 />\n",
    "</a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# sampleVariance\n",
    "x = sc.parallelize([1,3,2])\n",
    "y = x.sampleVariance()  # divides by N-1\n",
    "print(x.collect())\n",
    "print(y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a href=\"http://spark.apache.org/docs/1.2.0/api/python/pyspark.html#pyspark.RDD.countByValue\">\n",
    "<img align=left src=\"files/images/pyspark-page36.svg\" width=500 height=500 />\n",
    "</a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['t', 'h', 'e', ' ', 'b', 'i', 'g', ' ', 'b', 'a', 'd', ' ', 'w', 'o', 'l', 'f']\n",
      "defaultdict(<type 'int'>, {'a': 1, ' ': 3, 'b': 2, 'e': 1, 'd': 1, 'g': 1, 'f': 1, 'i': 1, 'h': 1, 'l': 1, 'o': 1, 't': 1, 'w': 1})\n"
     ]
    }
   ],
   "source": [
    "# countByValue\n",
    "x = sc.parallelize(list('the big bad wolf'))\n",
    "y = x.countByValue()\n",
    "print(x.collect())\n",
    "print(y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a href=\"http://spark.apache.org/docs/1.2.0/api/python/pyspark.html#pyspark.RDD.top\">\n",
    "<img align=left src=\"files/images/pyspark-page37.svg\" width=500 height=500 />\n",
    "</a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[1], [3], [1], [2, 3]]\n",
      "[1, 3, 1, 2, 3]\n",
      "[3, 3, 2]\n"
     ]
    }
   ],
   "source": [
    "# top\n",
    "x = sc.parallelize([1,3,1,2,3])\n",
    "print(x.glom().collect())\n",
    "y = x.top(num = 3)\n",
    "print(x.collect())\n",
    "print(y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a href=\"http://spark.apache.org/docs/1.2.0/api/python/pyspark.html#pyspark.RDD.takeOrdered\">\n",
    "<img align=left src=\"files/images/pyspark-page38.svg\" width=500 height=500 />\n",
    "</a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1, 3, 1, 2, 3]\n",
      "[1, 1, 2]\n"
     ]
    }
   ],
   "source": [
    "# takeOrdered\n",
    "x = sc.parallelize([1,3,1,2,3])\n",
    "y = x.takeOrdered(num = 3)\n",
    "print(x.collect())\n",
    "print(y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a href=\"http://spark.apache.org/docs/1.2.0/api/python/pyspark.html#pyspark.RDD.take\">\n",
    "<img align=left src=\"files/images/pyspark-page39.svg\" width=500 height=500 />\n",
    "</a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# take\n",
    "x = sc.parallelize([1,3,1,2,3])\n",
    "y = x.take(num = 3)\n",
    "print(x.collect())\n",
    "print(y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a href=\"http://spark.apache.org/docs/1.2.0/api/python/pyspark.html#pyspark.RDD.first\">\n",
    "<img align=left src=\"files/images/pyspark-page40.svg\" width=500 height=500 />\n",
    "</a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1, 3, 1, 2, 3]\n",
      "1\n"
     ]
    }
   ],
   "source": [
    "# first\n",
    "x = sc.parallelize([1,3,1,2,3])\n",
    "y = x.first()\n",
    "print(x.collect())\n",
    "print(y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# $Key-Value Pairs RDDs$\n",
    "\n",
    "> ### created by calling `.parallelize()` on a list of tuples."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "<a href=\"http://spark.apache.org/docs/1.2.0/api/python/pyspark.html#pyspark.RDD.collectAsMap\">\n",
    "<img align=left src=\"files/images/pyspark-page41.svg\" width=500 height=500 />\n",
    "</a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('C', 3), ('A', 1), ('B', 2)]\n",
      "{'A': 1, 'C': 3, 'B': 2}\n"
     ]
    }
   ],
   "source": [
    "# collectAsMap\n",
    "x = sc.parallelize([('C',3),('A',1),('B',2)])\n",
    "y = x.collectAsMap()\n",
    "print(x.collect())\n",
    "print(y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a href=\"http://spark.apache.org/docs/1.2.0/api/python/pyspark.html#pyspark.RDD.keys\">\n",
    "<img align=left src=\"files/images/pyspark-page42.svg\" width=500 height=500 />\n",
    "</a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('C', 3), ('A', 1), ('B', 2)]\n",
      "['C', 'A', 'B']\n"
     ]
    }
   ],
   "source": [
    "# keys\n",
    "x = sc.parallelize([('C',3),('A',1),('B',2)])\n",
    "y = x.keys()\n",
    "print(x.collect())\n",
    "print(y.collect())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a href=\"http://spark.apache.org/docs/1.2.0/api/python/pyspark.html#pyspark.RDD.values\">\n",
    "<img align=left src=\"files/images/pyspark-page43.svg\" width=500 height=500 />\n",
    "</a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# values\n",
    "x = sc.parallelize([('C',3),('A',1),('B',2)])\n",
    "y = x.values()\n",
    "print(x.collect())\n",
    "print(y.collect())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a href=\"http://spark.apache.org/docs/1.2.0/api/python/pyspark.html#pyspark.RDD.reduceByKey\">\n",
    "<img align=left src=\"files/images/pyspark-page44.svg\" width=500 height=500 />\n",
    "</a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# reduceByKey\n",
    "x = sc.parallelize([('B',1),('B',2),('A',3),('A',4),('A',5)])\n",
    "y = x.reduceByKey(lambda agg, obj: agg + obj)\n",
    "print(x.collect())\n",
    "print(y.collect())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "y"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a href=\"http://spark.apache.org/docs/1.2.0/api/python/pyspark.html#pyspark.RDD.reduceByKeyLocally\">\n",
    "<img align=left src=\"files/images/pyspark-page45.svg\" width=500 height=500 />\n",
    "</a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "x.reduceByKeyLocally?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[[('B', 1)], [('B', 2)], [('A', 3)], [('A', 4), ('A', 5)]]"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# reduceByKeyLocally\n",
    "x = sc.parallelize([('B',1),('B',2),('A',3),('A',4),('A',5)])\n",
    "x.glom().collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'A': 12, 'B': 3}"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y = x.reduceByKeyLocally(lambda a, b: a + b)\n",
    "y"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Another Example"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "str_1 = \"aaaaaaabbbbbbbcxxxxxxxsaxxxxxxzzzzzzxccccccccaaaaaassssss\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "kvp_rdd = sc.parallelize([(x, 1) for x in str_1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "4"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "kvp_rdd.getNumPartitions()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('a', 1), ('a', 1), ('a', 1), ('a', 1), ('a', 1), ('a', 1), ('a', 1), ('b', 1), ('b', 1), ('b', 1), ('b', 1), ('b', 1), ('b', 1), ('b', 1)] \n",
      "\n",
      "[('c', 1), ('x', 1), ('x', 1), ('x', 1), ('x', 1), ('x', 1), ('x', 1), ('x', 1), ('s', 1), ('a', 1), ('x', 1), ('x', 1), ('x', 1), ('x', 1)] \n",
      "\n",
      "[('x', 1), ('x', 1), ('z', 1), ('z', 1), ('z', 1), ('z', 1), ('z', 1), ('z', 1), ('x', 1), ('c', 1), ('c', 1), ('c', 1), ('c', 1), ('c', 1)] \n",
      "\n",
      "[('c', 1), ('c', 1), ('c', 1), ('a', 1), ('a', 1), ('a', 1), ('a', 1), ('a', 1), ('a', 1), ('s', 1), ('s', 1), ('s', 1), ('s', 1), ('s', 1), ('s', 1)] \n",
      "\n"
     ]
    }
   ],
   "source": [
    "for i in kvp_rdd.glom().collect():\n",
    "    print(i, '\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('a', 14), ('x', 14), ('c', 9), ('s', 7), ('b', 7), ('z', 6)]"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "kvp_rdd.reduceByKey(lambda x, y: x + y).collect()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a href=\"http://spark.apache.org/docs/1.2.0/api/python/pyspark.html#pyspark.RDD.countByKey\">\n",
    "<img align=left src=\"files/images/pyspark-page46.svg\" width=500 height=500 />\n",
    "</a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# countByKey\n",
    "x = sc.parallelize([('B',1),('B',2),('A',3),('A',4),('A',5)])\n",
    "y = x.countByKey()\n",
    "print(x.collect())\n",
    "print(y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a href=\"http://spark.apache.org/docs/1.2.0/api/python/pyspark.html#pyspark.RDD.join\">\n",
    "<img align=left src=\"files/images/pyspark-page47.svg\" width=500 height=500 />\n",
    "</a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('C', 4), ('B', 3), ('A', 2), ('A', 1)]\n",
      "[('A', 8), ('B', 7), ('A', 6), ('D', 5)]\n",
      "[('A', (2, 8)), ('A', (2, 6)), ('A', (1, 8)), ('A', (1, 6)), ('B', (3, 7))]\n"
     ]
    }
   ],
   "source": [
    "# join\n",
    "x = sc.parallelize([('C',4),('B',3),('A',2),('A',1)])\n",
    "y = sc.parallelize([('A',8),('B',7),('A',6),('D',5)])\n",
    "\n",
    "z = x.join(y)\n",
    "print(x.collect())\n",
    "print(y.collect())\n",
    "print(z.collect())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a href=\"http://spark.apache.org/docs/1.2.0/api/python/pyspark.html#pyspark.RDD.leftOuterJoin\">\n",
    "<img align=left src=\"files/images/pyspark-page48.svg\" width=500 height=500 />\n",
    "</a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# leftOuterJoin\n",
    "x = sc.parallelize([('C',4),('B',3),('A',2),('A',1)])\n",
    "y = sc.parallelize([('A',8),('B',7),('A',6),('D',5)])\n",
    "z = x.leftOuterJoin(y)\n",
    "print(x.collect())\n",
    "print(y.collect())\n",
    "print(z.collect())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a href=\"http://spark.apache.org/docs/1.2.0/api/python/pyspark.html#pyspark.RDD.rightOuterJoin\">\n",
    "<img align=left src=\"files/images/pyspark-page49.svg\" width=500 height=500 />\n",
    "</a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# rightOuterJoin\n",
    "x = sc.parallelize([('C',4),('B',3),('A',2),('A',1)])\n",
    "y = sc.parallelize([('A',8),('B',7),('A',6),('D',5)])\n",
    "z = x.rightOuterJoin(y)\n",
    "print(x.collect())\n",
    "print(y.collect())\n",
    "print(z.collect())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a href=\"http://spark.apache.org/docs/1.2.0/api/python/pyspark.html#pyspark.RDD.partitionBy\">\n",
    "<img align=left src=\"files/images/pyspark-page50.svg\" width=500 height=500 />\n",
    "</a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# partitionBy\n",
    "x = sc.parallelize([(0,1),(1,2),(2,3)], 2)\n",
    "print(x.glom().collect())\n",
    "\n",
    "y = x.partitionBy(numPartitions = 3, partitionFunc = lambda x: x)  \n",
    "# only key is passed to paritionFunc\n",
    "print(y.glom().collect())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a href=\"http://spark.apache.org/docs/1.2.0/api/python/pyspark.html#pyspark.RDD.combineByKey\">\n",
    "<img align=left src=\"files/images/pyspark-page51.svg\" width=500 height=500 />\n",
    "</a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# combineByKey\n",
    "x = sc.parallelize([('B',1),('B',2),('A',3),('A',4),('A',5)])\n",
    "\n",
    "createCombiner = (lambda el: [(el, el**2)]) \n",
    "\n",
    "mergeVal = (lambda aggregated, el: aggregated + [(el, el**2)]) # append to aggregated\n",
    "mergeComb = (lambda agg1,agg2: agg1 + agg2 )  # append agg1 with agg2\n",
    "\n",
    "y = x.combineByKey(createCombiner, mergeVal, mergeComb)\n",
    "\n",
    "print(x.collect())\n",
    "print(y.collect())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a href=\"http://spark.apache.org/docs/1.2.0/api/python/pyspark.html#pyspark.RDD.aggregateByKey\">\n",
    "<img align=left src=\"files/images/pyspark-page52.svg\" width=500 height=500 />\n",
    "</a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# aggregateByKey\n",
    "x = sc.parallelize([('B',1),('B',2),('A',3),('A',4),('A',5)])\n",
    "\n",
    "zeroValue = [] # empty list is 'zero value' for append operation\n",
    "mergeVal = (lambda aggregated, el: aggregated + [(el,el**2)])\n",
    "mergeComb = (lambda agg1,agg2: agg1 + agg2 )\n",
    "\n",
    "y = x.aggregateByKey(zeroValue,mergeVal,mergeComb)\n",
    "print(x.collect())\n",
    "print(y.collect())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a href=\"http://spark.apache.org/docs/1.2.0/api/python/pyspark.html#pyspark.RDD.foldByKey\">\n",
    "<img align=left src=\"files/images/pyspark-page53.svg\" width=500 height=500 />\n",
    "</a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# foldByKey\n",
    "x = sc.parallelize([('B',1),('B',2),('A',3),('A',4),('A',5)])\n",
    "zeroValue = 1 # one is 'zero value' for multiplication\n",
    "y = x.foldByKey(zeroValue,lambda agg, x: agg*x )  # computes cumulative product within each key\n",
    "print(x.collect())\n",
    "print(y.collect())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a href=\"http://spark.apache.org/docs/1.2.0/api/python/pyspark.html#pyspark.RDD.groupByKey\">\n",
    "<img align=left src=\"files/images/pyspark-page54.svg\" width=500 height=500 />\n",
    "</a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# groupByKey\n",
    "x = sc.parallelize([('B',5),('B',4),('A',3),('A',2),('A',1)])\n",
    "y = x.groupByKey()\n",
    "print(x.collect())\n",
    "print([(j[0],[i for i in j[1]]) for j in y.collect()])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a href=\"http://spark.apache.org/docs/1.2.0/api/python/pyspark.html#pyspark.RDD.flatMapValues\">\n",
    "<img align=left src=\"files/images/pyspark-page55.svg\" width=500 height=500 />\n",
    "</a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# flatMapValues\n",
    "x = sc.parallelize([('A',(1,2,3)),('B',(4,5))])\n",
    "y = x.flatMapValues(lambda x: [i**2 for i in x]) \n",
    "# function is applied to entire value, then result is flattened\n",
    "print(x.collect())\n",
    "print(y.collect())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a href=\"http://spark.apache.org/docs/1.2.0/api/python/pyspark.html#pyspark.RDD.mapValues\">\n",
    "<img align=left src=\"files/images/pyspark-page56.svg\" width=500 height=500 />\n",
    "</a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# mapValues\n",
    "x = sc.parallelize([('A',(1,2,3)),('B',(4,5))])\n",
    "y = x.mapValues(lambda x: [i**2 for i in x]) \n",
    "# function is applied to entire value\n",
    "print(x.collect())\n",
    "print(y.collect())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a href=\"http://spark.apache.org/docs/1.2.0/api/python/pyspark.html#pyspark.RDD.groupWith\">\n",
    "<img align=left src=\"files/images/pyspark-page57.svg\" width=500 height=500 />\n",
    "</a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Alias for cogroup but with support for multiple RDDs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# groupWith\n",
    "x = sc.parallelize([('C',4),('B',(3,3)),('A',2),('A',(1,1))])\n",
    "y = sc.parallelize([('B',(7,7)),('A',6),('D',(5,5))])\n",
    "z = sc.parallelize([('D',9),('B',(8,8))])\n",
    "print(x.collect())\n",
    "print(y.collect())\n",
    "print(z.collect())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "a = x.groupWith(y, z)\n",
    "\n",
    "print(\"Result:\")\n",
    "for key,val in list(a.collect()): \n",
    "    print(key, [list(i) for i in val])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a href=\"http://spark.apache.org/docs/1.2.0/api/python/pyspark.html#pyspark.RDD.cogroup\">\n",
    "<img align=left src=\"files/images/pyspark-page58.svg\" width=500 height=500 />\n",
    "</a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# cogroup\n",
    "x = sc.parallelize([('C',4),('B',(3,3)),('A',2),('A',(1,1))])\n",
    "y = sc.parallelize([('A',8),('B',7),('A',6),('D',(5,5))])\n",
    "z = x.cogroup(y)\n",
    "print(x.collect())\n",
    "print(y.collect())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "for key,val in list(z.collect()):\n",
    "    print(key, [list(i) for i in val])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a href=\"http://spark.apache.org/docs/1.2.0/api/python/pyspark.html#pyspark.RDD.sampleByKey\">\n",
    "<img align=left src=\"files/images/pyspark-page59.svg\" width=500 height=500 />\n",
    "</a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# sampleByKey\n",
    "x = sc.parallelize([('A',1),('B',2),('C',3),('B',4),('A',5)])\n",
    "y = x.sampleByKey(withReplacement=False, fractions={'A':0.5, 'B':1, 'C':0.2})\n",
    "print(x.collect())\n",
    "print(y.collect())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a href=\"http://spark.apache.org/docs/1.2.0/api/python/pyspark.html#pyspark.RDD.subtractByKey\">\n",
    "<img align=left src=\"files/images/pyspark-page60.svg\" width=500 height=500 />\n",
    "</a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# subtractByKey\n",
    "x = sc.parallelize([('C',1),('B',2),('A',3),('A',4)])\n",
    "y = sc.parallelize([('A',5),('D',6),('A',7),('D',8)])\n",
    "\n",
    "z = x.subtractByKey(y)\n",
    "v = y.subtractByKey(x)\n",
    "\n",
    "print(x.collect())\n",
    "print(y.collect())\n",
    "print(z.collect())\n",
    "print(v.collect())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a href=\"http://spark.apache.org/docs/1.2.0/api/python/pyspark.html#pyspark.RDD.subtract\">\n",
    "<img align=left src=\"files/images/pyspark-page61.svg\" width=500 height=500 />\n",
    "</a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# subtract\n",
    "x = sc.parallelize([('C',4),('B',3),('A',2),('A',1)])\n",
    "y = sc.parallelize([('C',8),('A',2),('D',1)])\n",
    "z = x.subtract(y)\n",
    "print(x.collect())\n",
    "print(y.collect())\n",
    "print(z.collect())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a href=\"http://spark.apache.org/docs/1.2.0/api/python/pyspark.html#pyspark.RDD.keyBy\">\n",
    "<img align=left src=\"files/images/pyspark-page62.svg\" width=500 height=500 />\n",
    "</a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# keyBy\n",
    "x = sc.parallelize([1,2,3])\n",
    "y = x.keyBy(lambda x: x**2)\n",
    "\n",
    "print(x.collect())\n",
    "print(y.collect())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "x.map(lambda x: (x**2, x)).collect()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a href=\"http://spark.apache.org/docs/1.2.0/api/python/pyspark.html#pyspark.RDD.repartition\">\n",
    "<img align=left src=\"files/images/pyspark-page63.svg\" width=500 height=500 />\n",
    "</a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# repartition\n",
    "x = sc.parallelize([1,2,3,4,5], 2)\n",
    "print(x.glom().collect())\n",
    "\n",
    "\n",
    "y = x.repartition(numPartitions=3)\n",
    "print(y.glom().collect())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a href=\"http://spark.apache.org/docs/1.2.0/api/python/pyspark.html#pyspark.RDD.coalesce\">\n",
    "<img align=left src=\"files/images/pyspark-page64.svg\" width=500 height=500 />\n",
    "</a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# coalesce\n",
    "x = sc.parallelize([1,2,3,4,5], 2)\n",
    "y = x.coalesce(numPartitions=1)\n",
    "\n",
    "print(x.glom().collect())\n",
    "print(y.glom().collect())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a href=\"http://spark.apache.org/docs/1.2.0/api/python/pyspark.html#pyspark.RDD.zip\">\n",
    "<img align=left src=\"files/images/pyspark-page65.svg\" width=500 height=500 />\n",
    "</a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# zip\n",
    "x = sc.parallelize(['B','A','A'])\n",
    "y = x.map(lambda x: ord(x))  # zip expects x and y to have same #partitions and #elements/partition\n",
    "z = x.zip(y)\n",
    "print(x.collect())\n",
    "print(y.collect())\n",
    "print(z.collect())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a href=\"http://spark.apache.org/docs/1.2.0/api/python/pyspark.html#pyspark.RDD.zipWithIndex\">\n",
    "<img align=left src=\"files/images/pyspark-page66.svg\" width=500 height=500 />\n",
    "</a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# zipWithIndex\n",
    "x = sc.parallelize(['B','A','A', 'C', 'C', 'A', 'C'],2)\n",
    "y = x.zipWithIndex()\n",
    "print(x.glom().collect())\n",
    "print(y.collect())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a href=\"http://spark.apache.org/docs/1.2.0/api/python/pyspark.html#pyspark.RDD.zipWithUniqueId\">\n",
    "<img align=left src=\"files/images/pyspark-page67.svg\" width=500 height=500 />\n",
    "</a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# zipWithUniqueId\n",
    "x = sc.parallelize(['B','A','A', 'A', 'C', 'B'],2)\n",
    "y = x.zipWithUniqueId()\n",
    "print(x.glom().collect())\n",
    "print(y.collect())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
