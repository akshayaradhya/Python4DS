{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# `spark.sql` & `Dataframes`\n",
    "---\n",
    "\n",
    "The Spark RDD API has basic functionality for performing data analysis tasks. \n",
    "\n",
    "spark.sql has interfaces for capturing information about the structure of the data and the computations being performed, and it uses this information for optimizing tasks.\n",
    "\n",
    "Ways to interact with spark.sql include \n",
    "\n",
    "- SQL (basic SQL and/or HiveQL)\n",
    "- DataFrames API,\n",
    "- Datasets API\n",
    "\n",
    "The execution engine used for a particular task is the same irrespective of the API. This gives developers the freedom to choose the API that most naturally expresses a given computation.\n",
    "\n",
    "---\n",
    "## `SQL`\n",
    "\n",
    "- to execute SQL queries written using either a basic SQL syntax or HiveQL.\n",
    "- to read data from Hive or other databases over ODBC/JDBC\n",
    "- query results are returned as DataFrames\n",
    "\n",
    "---\n",
    "## `DataFrames`\n",
    "\n",
    "- a distributed collection of data with named columns. \n",
    "- conceptually equivalent to RDBMS tables or data-frames in R/Python, (but with richer optimizations under the hood.)\n",
    "- can be constructed from: \n",
    "    - structured text/csv files, \n",
    "    - Hive tables, \n",
    "    - external databases, or  \n",
    "    - existing RDDs\n",
    "- available in Scala, Java, Python, and R.\n",
    "\n",
    "---\n",
    "## `Datasets`\n",
    "\n",
    "- new experimental interface added in Spark 1.6 \n",
    "- tries to provide the benefits of RDDs (strong typing, lambda functions) with the benefits of Spark SQL’s optimized execution engine. \n",
    "- can be constructed from JVM objects and then manipulated using functional transformations (map, flatMap, filter, etc.)\n",
    "- available in Scala and Java only\n",
    "\n",
    "\n",
    "---\n",
    "## `SQLContext` and `HiveContext`\n",
    "\n",
    "This is the entry point into all relational functionality in Spark. \n",
    "\n",
    "A basic `SQLContext`, is created using a `SparkContext`\n",
    "\n",
    "```\n",
    "from pyspark.sql import SQLContext\n",
    "sqlContext = SQLContext(sc)\n",
    "```\n",
    "\n",
    "The `HiveContext` provides additional features, including the ability to write queries using the more complete HiveQL parser, access to Hive UDFs, and the ability to read data from Hive tables."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from pandas import DataFrame\n",
    "import numpy as np\n",
    "sc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from pyspark.sql import SQLContext\n",
    "sqlctx = SQLContext(sc)\n",
    "\n",
    "type(sqlctx)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# explore methods\n",
    "spark"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "pandas_df = DataFrame(data=np.random.randint(1, 100, 20).reshape(4, 5),\n",
    "                     columns=list('ABCDE'))\n",
    "pandas_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "\n",
    "spark = SparkSession \\\n",
    "    .builder \\\n",
    "    .appName(\"Python Spark SQL basic example\") \\\n",
    "    .config(\"spark.some.config.option\", \"some-value\") \\\n",
    "    .getOrCreate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "spark.createDataFrame(pandas_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "spark_df = spark.createDataFrame(pandas_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "type(spark_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "spark_df.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3. DataFrame\n",
    "\n",
    "Like an RDD, a DataFrame is an immutable distributed collection of data. \n",
    "\n",
    "Unlike an RDD, data is organized into named columns, like a table in a relational database. \n",
    "\n",
    "Designed to make large data sets processing even easier, DataFrame allows developers to impose a structure onto a distributed collection of data, allowing higher-level abstraction; it provides a domain specific (pandas-like) language API to manipulate your distributed data; and makes Spark accessible to a wider audience, beyond specialized data engineers.\n",
    "\n",
    "Below is an Apache Spark code snippet using SQL and DataFrames to query and join different data sources.\n",
    "\n",
    "```\n",
    "# Read JSON file and register temp view\n",
    "context.jsonFile(\"s3n://...\").createOrReplaceTempView(\"json\")\n",
    " \n",
    "# Execute SQL query \n",
    "results = context.sql(\"\"\"SELECT * FROM people JOIN json ...\"\"\")\n",
    "```\n",
    "\n",
    "---\n",
    "## A bit of history\n",
    "The developers of Apache Spark aimed to provide a simple API for distributed data processing in general-purpose programming languages (Java, Python, Scala, R). Earlier versions of Spark enabled distributed data processing through functional transformations on distributed collections of data (RDDs). This was an incredibly powerful API: tasks that used to take thousands of lines of code to express could be reduced to dozens.\n",
    "\n",
    "As Spark continued to grow, we want to enable wider audiences beyond “Big Data” engineers to leverage the power of distributed processing. The new DataFrames API was created with this goal in mind.  This API is inspired by data frames in R and Python (Pandas), but designed from the ground-up to support modern big data and data science applications. As an extension to the existing RDD API, DataFrames feature:\n",
    "\n",
    "- Ability to scale from kilobytes of data on a single laptop to petabytes on a large cluster\n",
    "- Support for a wide array of data formats and storage systems\n",
    "- State-of-the-art optimization and code generation through the Spark SQL Catalyst optimizer\n",
    "- Seamless integration with all big data tooling and infrastructure via Spark\n",
    "- APIs for Python, Java, Scala, and R\n",
    "\n",
    "For new users familiar with data frames in other programming languages, this API makes them feel at home. For existing Spark users, this extended API will make Spark easier to program, and at the same time improve performance through intelligent optimizations and code-generation.\n",
    "\n",
    "The introduction of Dataframe is actually kind of a **big deal**, because when RDDs were the only option to load data, it was obvious that you needed to \n",
    "\n",
    "- parse your *maybe* un-structured data using RDDs, \n",
    "- transform them using case-classes or tuples and then \n",
    "- do the special work that you actually needed. \n",
    "\n",
    "Spark SQL is not a new project and one could, of course, load structured-data (like Parquet files) directly from a SQLContext before Spark 1.3 – but the advantages were limited to running SQL queries or exposing a JDBC-compatible server for other BI tools.\n",
    "\n",
    "---\n",
    "## Advantages of Dataframes\n",
    "\n",
    "### 1) Dataframes are a higher level of abstraction than RDDs\n",
    "\n",
    "If you’re familiar with Pandas syntax, you will feel at home using Spark’s Dataframe and even if you’re not, you’ll learn and – I’d even add – learn to love it. Why ? Because it’s a higher level of programming than the RDD, you can do more, faster\n",
    "\n",
    "<img src=\"https://ogirardot.files.wordpress.com/2015/05/rdd-vs-dataframe.png?w=640&h=360\">\n",
    "\n",
    "\n",
    "### 2) Spark SQL/Catalyst is intelligent \n",
    "\n",
    "When you’re using Dataframe, you’re not defining directly a DAG (Directed Acyclic Graph) anymore, you’re actually creating an AST (Abstract Syntax Tree) that the Catalyst engine will parse, check and improve using both Rules-Based Optimisation and Cost-Based Optimisation.\n",
    "\n",
    "### 3) Python & Scala are now even in terms of performance\n",
    "\n",
    "Using the Dataframe API, you’re using a DSL that leverages Spark’s Scala bytecode – when using RDDs, Python lambdas will run in a Python VM, Java/Scala lambdas will run in the JVM, this is great because inside RDDs you can use your usual Python libraries (Numpy, Scipy, etc…) and not some Jython code, but it comes at a performance cost.\n",
    "\n",
    "This is still true if you want to use Dataframe’s User Defined Functions, you can write them in Java/Scala or Python and this will impact your computation performance – but if you manage to stay in a pure Dataframe computation – then nothing will get between you and the best computation performance you can possibly get.\n",
    "\n",
    "### 4)  Dataframes are the future for Spark\n",
    "\n",
    "Spark ML is already a pretty obvious example of this, the Pipeline API is designed entirely around Dataframes as their sole data structure for parallel computations, model training and predictions.\n",
    "\n",
    "Here's what the future of Spark looks like:\n",
    "\n",
    "<img src=\"https://ogirardot.files.wordpress.com/2015/05/future-of-spark.png?w=640&h=358\">\n",
    "\n",
    "\n",
    "---\n",
    "## What Are DataFrames?\n",
    "In Spark, a DataFrame is a distributed collection of data organized into named columns. It is conceptually equivalent to a table in a relational database or a data frame in R/Python, but with richer optimizations under the hood. DataFrames can be constructed from a wide array of sources such as: structured data files, tables in Hive, external databases, or existing RDDs.\n",
    "\n",
    "How Can One Use DataFrames?\n",
    "Once built, DataFrames provide a domain-specific language for distributed data manipulation.  Here is an example of using DataFrames to manipulate the demographic data of a large population of users:\n",
    "\n",
    "```\n",
    "# Create a new DataFrame that contains “young users” only\n",
    "young = users.filter(users.age < 21)\n",
    " \n",
    "# Alternatively, using Pandas-like syntax\n",
    "young = users[users.age < 21]\n",
    " \n",
    "# Increment everybody’s age by 1\n",
    "young.select(young.name, young.age + 1)\n",
    " \n",
    "# Count the number of young users by gender\n",
    "young.groupBy(\"gender\").count()\n",
    " \n",
    "# Join young users with another DataFrame called logs\n",
    "young.join(logs, young.userId == logs.userId, \"left_outer\")\n",
    "```\n",
    "\n",
    "---\n",
    "## Using SQL\n",
    "You can also incorporate SQL while working with DataFrames, using Spark SQL. This example counts the number of users in the young DataFrame.\n",
    "\n",
    "```\n",
    "young.registerTempTable(\"young\")\n",
    "context.sql(\"SELECT count(*) FROM young\")\n",
    "```\n",
    "---\n",
    "## Pandas and Spark\n",
    "In Python, you can also convert freely between Pandas DataFrame and Spark DataFrame:\n",
    "\n",
    "```\n",
    "# Convert Spark DataFrame to Pandas\n",
    "pandas_df = young.toPandas()\n",
    " \n",
    "# Create a Spark DataFrame from Pandas\n",
    "spark_df = context.createDataFrame(pandas_df)\n",
    "```\n",
    "\n",
    "Similar to RDDs, DataFrames are evaluated lazily. That is to say, computation only happens when an action (e.g. display result, save output) is required. This allows their executions to be optimized, by applying techniques such as predicate push-downs and bytecode generation. All DataFrame operations are also automatically parallelized and distributed on clusters.\n",
    "\n",
    "---\n",
    "## Supported Data Formats and Sources\n",
    "Modern applications often need to collect and analyze data from a variety of sources. Out of the box, DataFrame supports reading data from the most popular formats, including JSON files, Parquet files, Hive tables. It can read from local file systems, distributed file systems (HDFS), cloud storage (S3), and external relational database systems via JDBC. In addition, through Spark SQL’s external data sources API, DataFrames can be extended to support any third-party data formats or sources. Existing third-party extensions already include Avro, CSV, ElasticSearch, and Cassandra.\n",
    "\n",
    "<img src=\"https://databricks.com/wp-content/uploads/2015/02/Introducing-DataFrames-in-Spark-for-Large-Scale-Data-Science1.png\">\n",
    "\n",
    "\n",
    "---\n",
    "## Combine data from Disparate Sources\n",
    "DataFrames’ support for data sources enables applications to easily combine data from disparate sources (known as federated query processing in database systems). For example, the following code snippet joins a site’s textual traffic log stored in S3 with a PostgreSQL database to count the number of times each user has visited the site.\n",
    "\n",
    "```\n",
    "users = context.jdbc(\"jdbc:postgresql:production\", \"users\")\n",
    "logs = context.load(\"/path/to/traffic.log\")\n",
    "logs.join(users, logs.userId == users.userId, \"left_outer\") \\\n",
    "  .groupBy(\"userId\").agg({\"*\": \"count\"})\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "# In-Depth: Sparkling Pandas\n",
    "\n",
    "you can finally port pretty much any relevant piece of Pandas’ DataFrame computation to Apache Spark parallel computation framework using Spark SQL’s DataFrame. Let's see how we can take a few concepts from Pandas DataFrame and see how we can translate this to PySpark’s DataFrame.\n",
    "\n",
    "> Disclaimer:  a few operations that you can do in Pandas don’t translate to Spark well. Please remember that DataFrames in Spark are like RDD in the sense that they’re an immutable data structure. \n",
    "\n",
    "Therefore things like:\n",
    "\n",
    "```\n",
    "# Pandas code to create a new column \"three\"\n",
    "df['three'] = df['one'] * df['two']\n",
    "```\n",
    "\n",
    "can’t exist, just because this kind of affectation goes against the principles of Spark. Another example would be trying to access by index a single element within a DataFrame. Don’t forget that you’re using a distributed data structure, not an in-memory random-access data structure.\n",
    "\n",
    "To be clear, this doesn’t mean that you can’t do the same kind of thing (i.e. create a new column) using Spark, it means that you have to think immutable/distributed and re-write parts of your code, mostly the parts that are not purely thought of as transformations on a stream of data.\n",
    "\n",
    "## 0. Creating Toy DataFrames in Pandas and Spark"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Pandas => pdf\n",
    "import pandas as pd\n",
    "pdf = pd.DataFrame(data=[[1, 2, 3], [4, 5, 6]], \n",
    "                   columns=['A', 'B', 'C'])\n",
    "pdf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# SPARK SQL => df\n",
    "df = spark.createDataFrame(data=[(1, 4), \n",
    "                                 (2, 5), \n",
    "                                 (3, 6)], \n",
    "                            schema=[\"A\", \"B\"])\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "sparkcreateDataFrame()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "df.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Subsetting Columns\n",
    "\n",
    "This part is not that much different in Pandas and Spark, but you have to take into account the immutable character of your DataFrame. First let’s create two DataFrames one in Pandas *pdf* and one in Spark *df*\n",
    "\n",
    "In Spark SQL or Pandas you use the same syntax to refer to a column. \n",
    "\n",
    "> The only difference is that in Pandas, it is a mutable data structure that you can change – not in Spark."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Pandas\n",
    "print pdf.A\n",
    "print pdf['A']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Spark\n",
    "df.A"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "df['A']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "type(df['A'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# pyspark.sql.Row\n",
    "# pyspark.sql.Column"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## 2. Column adding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Pandas\n",
    "pdf['D'] = 0\n",
    "pdf"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In Spark SQL you'll use the `withColumn` or the `select` method, but you need to create a \"Column\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Spark\n",
    "from pyspark.sql import functions as F"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "df.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "df.withColumn('C', F.lit(0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "df.withColumn('C', F.lit(0)).show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "NOTE\n",
    "\n",
    "Most of the time in Spark SQL you can use Strings to reference columns but there are cases where you’ll want to use the Column objects rather than Strings :\n",
    "\n",
    "- When you need to manipulate columns using expressions like Adding two columns to each other, Twice the value of this column or even Is the column value larger than 0 ?, you won’t be able to use simple strings and will need the Column reference.\n",
    "- Finally if you need renaming, cast or any other complex feature, you’ll need the Column reference too.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "When you’re selecting columns, to create another projected DataFrame, you can also use expressions:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Creating new columns using `withColumn()` and `select`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "df.withColumn('C', df.A * 2).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "df.withColumn('C', df.B > 4).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "df.select(df.B > 0).show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The column name will actually be computed according to the expression you defined. \n",
    "\n",
    "\n",
    "If you want to rename this, you’ll need to use the alias method on Column:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "df.select((df.B > 4).alias(\"is_grt_4\")).show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "All of the expressions that we’re building here can be used for \n",
    "\n",
    "- Filtering, \n",
    "- Adding a new column or \n",
    "- even inside Aggregations"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Split-Apply-Combine\n",
    "\n",
    "What can be confusing at first in using aggregations is that the minute you write `groupBy` you're not using a DataFrame object, you’re actually using a `GroupedData` object and you need to precise your aggregations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "pdf_2 = DataFrame()\n",
    "pdf_2['A'] = list('XY' * 5)\n",
    "pdf_2['B'] = np.random.randn(10).round(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "pdf_2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "df = spark.createDataFrame(pdf_2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "g_df = df.groupBy(\"A\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "g_df.avg(\"B\").show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "df.groupBy(\"A\").avg(\"B\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "df.groupBy(\"A\").avg(\"B\").show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "df.groupBy('A').max().show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If you need only one aggregation, you can use the simplest functions like: `avg, cout, max, min, mean` and `sum` directly on `GroupedData`, but most of the time, this will be too simple and you’ll want **to create a few aggregations during a single `groupBy` operation.** \n",
    "\n",
    "To do so you’ll be using the `agg` method, just like we do in Pandas:\n",
    "- This is one of the greatest features of the DataFrames. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "df.groupBy(\"A\").agg(F.avg(\"B\"), F.min(\"B\"), F.max(\"B\")).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "df.groupBy(\"A\").agg(\n",
    "    F.first(\"B\").alias(\"first\"),\n",
    "    F.last(\"B\").alias(\"last\"),\n",
    "    F.sum(\"B\").alias(\"sum\")\n",
    ").show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- The entry point into all SQL functionality in Spark is the `SQLContext` class. \n",
    "- To create a basic instance, all we need is a `SparkContext` reference."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from pyspark.sql import SQLContext\n",
    "sqlContext = SQLContext(sc)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> ## Creating a Spark DataFrame from a Row-RDD\n",
    "\n",
    "- Take an existing RDD\n",
    "- Convert (using `map` and `Row()`) it into a Row-RDD \n",
    "- Call the createDataFrame() funtion on the Row-RDD\n",
    "\n",
    "\n",
    "---\n",
    "### 1. Specifying the Schema and registering as a Table\n",
    "\n",
    "With a SQLContext, we are ready to create a DataFrame from our existing RDD. \n",
    "But first we need to tell Spark SQL the schema in our data.\n",
    "\n",
    "Spark SQL can convert an RDD of Row objects to a DataFrame. \n",
    "Rows are constructed by passing a list of key/value pairs as kwargs to the Row class. \n",
    "- The keys define the column names, and \n",
    "- the types are inferred by looking at the first row. \n",
    "\n",
    "> Therefore, it is important that there is no missing data in the first row of the RDD in order to properly infer the schema.\n",
    "\n",
    "In our case, we first need \n",
    "- to split the comma separated data, and then \n",
    "- use the information in KDD's 1999 task description to obtain the column names.\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from pyspark.sql import Row\n",
    "Row?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "row_1 = Row(C1 = 'Pi', C2 = 3.142)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "row_2 = Row(C1 = 'e', C2 = 2.7)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "spark.createDataFrame([row_1, row_2]).show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Example: Analysing the KDD Cup Data\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "data_file = '/home/dush/Spark/kddcup.data_10_percent'\n",
    "raw_data = sc.textFile(data_file).cache()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "type(raw_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "raw_data.is_cached"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "raw_data.take(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "csv_data = raw_data.map(lambda l: l.split(\",\"))\n",
    "\n",
    "row_data = csv_data.map(lambda p: Row(duration=int(p[0]), \n",
    "                                      protocol_type=p[1],\n",
    "                                      service=p[2],\n",
    "                                      flag=p[3],\n",
    "                                      src_bytes=int(p[4]),\n",
    "                                      dst_bytes=int(p[5])))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "type(row_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "row_data.take(2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Once we have our RDD of Row we can infer and register the schema."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> ### Creating the Spark DF from an RDD of Row Objects"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "interactions_df = spark.createDataFrame(row_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "interactions_df.show(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "interactions_df.count()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> ### Registering the Spark DF as a Temp Table to run SQL queries on it"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "interactions_df.registerTempTable(\"interactions\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------+--------+----+-------------+-------+---------+\n",
      "|dst_bytes|duration|flag|protocol_type|service|src_bytes|\n",
      "+---------+--------+----+-------------+-------+---------+\n",
      "|     5450|       0|  SF|          tcp|   http|      181|\n",
      "|      486|       0|  SF|          tcp|   http|      239|\n",
      "|     1337|       0|  SF|          tcp|   http|      235|\n",
      "|     1337|       0|  SF|          tcp|   http|      219|\n",
      "|     2032|       0|  SF|          tcp|   http|      217|\n",
      "|     2032|       0|  SF|          tcp|   http|      217|\n",
      "|     1940|       0|  SF|          tcp|   http|      212|\n",
      "|     4087|       0|  SF|          tcp|   http|      159|\n",
      "|      151|       0|  SF|          tcp|   http|      210|\n",
      "|      786|       0|  SF|          tcp|   http|      212|\n",
      "+---------+--------+----+-------------+-------+---------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "spark.sql(\"SELECT * FROM interactions LIMIT 10\").show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------+\n",
      "|count(1)|\n",
      "+--------+\n",
      "|  494021|\n",
      "+--------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "spark.sql(\"SELECT count(*) FROM interactions\").show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- We can easily have a look at our data frame schema using `printSchema`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- dst_bytes: long (nullable = true)\n",
      " |-- duration: long (nullable = true)\n",
      " |-- flag: string (nullable = true)\n",
      " |-- protocol_type: string (nullable = true)\n",
      " |-- service: string (nullable = true)\n",
      " |-- src_bytes: long (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "interactions_df.printSchema()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Now we can run SQL queries over our data frame that has been registered as a table. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "139"
      ]
     },
     "execution_count": 65,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Select tcp network interactions with \n",
    "# more than 1 second duration and \n",
    "# no transfer from destination\n",
    "\n",
    "tcp_interactions = spark.sql(\"\"\"\n",
    "    SELECT duration, dst_bytes \n",
    "    FROM interactions \n",
    "    WHERE protocol_type = 'tcp' \n",
    "    AND duration > 1000 \n",
    "    AND dst_bytes = 0\n",
    "\"\"\")\n",
    "\n",
    "tcp_interactions.count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "pyspark.sql.dataframe.DataFrame"
      ]
     },
     "execution_count": 66,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "type(tcp_interactions)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- The results of SQL queries are DFs and support all the normal DF functions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Output duration together with dst_bytes\n",
    "\n",
    "tcp_interactions_out = (tcp_interactions\n",
    "                        .rdd\n",
    "                        .map(lambda p: \"Duration: {}, Dest. bytes: {}\".format(p.duration, p.dst_bytes)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Duration: 5057, Dest. bytes: 0', 'Duration: 5059, Dest. bytes: 0']"
      ]
     },
     "execution_count": 68,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tcp_interactions_out.take(2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Spark DataFrame provides a **domain-specific language** for structured data manipulation. \n",
    "- This language includes _methods_ we can concatenate (or chain) in order to do `selection, filtering, grouping` etc. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2. Count interactions by protocol type"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------------+------------------+------------------+\n",
      "|protocol_type|     avg(duration)|    avg(dst_bytes)|\n",
      "+-------------+------------------+------------------+\n",
      "|          tcp|18.299576460684502|2248.4364612106383|\n",
      "|          udp|   993.64616291638| 84.70968851331433|\n",
      "|         icmp|               0.0|               0.0|\n",
      "+-------------+------------------+------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "(interactions_df\n",
    " .select(\"protocol_type\", \"duration\", \"dst_bytes\")\n",
    " .groupBy(\"protocol_type\")\n",
    " .mean()\n",
    " .show()\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {
    "collapsed": false,
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------------+------------------+------------------+\n",
      "|protocol_type|     avg(duration)|    avg(dst_bytes)|\n",
      "+-------------+------------------+------------------+\n",
      "|          tcp|18.299576460684502|2248.4364612106383|\n",
      "|          udp|   993.64616291638| 84.70968851331433|\n",
      "|         icmp|               0.0|               0.0|\n",
      "+-------------+------------------+------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "interactions_df.registerTempTable(\"table_1\")\n",
    "\n",
    "spark.sql(\"SELECT protocol_type, avg(duration), avg(dst_bytes) from table_1 group by protocol_type\").show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### the `describe()` method"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+-----------------+-----------------+------+-------------+-------+------------------+\n",
      "|summary|        dst_bytes|         duration|  flag|protocol_type|service|         src_bytes|\n",
      "+-------+-----------------+-----------------+------+-------------+-------+------------------+\n",
      "|  count|           494021|           494021|494021|       494021| 494021|            494021|\n",
      "|   mean|868.5324247349809|47.97930249928647|  null|         null|   null|3025.6102959185946|\n",
      "| stddev|33040.00125210233|707.7464723053739|  null|         null|   null| 988218.1010503998|\n",
      "|    min|                0|                0|   OTH|         icmp|    IRC|                 0|\n",
      "|    max|          5155468|            58329|    SH|          udp|  whois|         693375640|\n",
      "+-------+-----------------+-----------------+------+-------------+-------+------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print interactions_df.describe().show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "spark.sql(\"\"\"select flag, count(*) \n",
    "                 from interactions \n",
    "                group by flag\"\"\").show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# %%timeit\n",
    "# count how many interactions last more than 1 second, \n",
    "# with no data transfer from destination, grouped by protocol type\n",
    "\n",
    "(interactions_df\n",
    " .select(\"protocol_type\", \"duration\", \"dst_bytes\")\n",
    " .filter((interactions_df.duration > 1000) & (interactions_df.dst_bytes == 0))\n",
    " .groupBy(\"protocol_type\")\n",
    " .count()\n",
    " .show()\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# %%timeit\n",
    "spark.sql(\"\"\"SELECT protocol_type, count(*) \n",
    "           from interactions \n",
    "           where duration > 1000 \n",
    "           and dst_bytes = 0 \n",
    "           group by protocol_type\"\"\").show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3. Count number of attack and normal interactions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# First we need to add the label column to our data.\n",
    "\n",
    "def get_label_type(label):\n",
    "    if label!=\"normal.\":\n",
    "        return \"attack\"\n",
    "    else:\n",
    "        return \"normal\"\n",
    "\n",
    "row_labeled_data = csv_data.map(lambda p: Row(duration=int(p[0]), \n",
    "                                              protocol_type=p[1],\n",
    "                                              service=p[2],\n",
    "                                              flag=p[3],\n",
    "                                              src_bytes=int(p[4]),\n",
    "                                              dst_bytes=int(p[5]),\n",
    "                                              label=get_label_type(p[41])))\n",
    "\n",
    "interactions_labeled_df = spark.createDataFrame(row_labeled_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------+--------+----+------+-------------+-------+---------+\n",
      "|dst_bytes|duration|flag| label|protocol_type|service|src_bytes|\n",
      "+---------+--------+----+------+-------------+-------+---------+\n",
      "|     5450|       0|  SF|normal|          tcp|   http|      181|\n",
      "|      486|       0|  SF|normal|          tcp|   http|      239|\n",
      "|     1337|       0|  SF|normal|          tcp|   http|      235|\n",
      "|     1337|       0|  SF|normal|          tcp|   http|      219|\n",
      "|     2032|       0|  SF|normal|          tcp|   http|      217|\n",
      "|     2032|       0|  SF|normal|          tcp|   http|      217|\n",
      "|     1940|       0|  SF|normal|          tcp|   http|      212|\n",
      "|     4087|       0|  SF|normal|          tcp|   http|      159|\n",
      "|      151|       0|  SF|normal|          tcp|   http|      210|\n",
      "|      786|       0|  SF|normal|          tcp|   http|      212|\n",
      "|      624|       0|  SF|normal|          tcp|   http|      210|\n",
      "|     1985|       0|  SF|normal|          tcp|   http|      177|\n",
      "|      773|       0|  SF|normal|          tcp|   http|      222|\n",
      "|     1169|       0|  SF|normal|          tcp|   http|      256|\n",
      "|      259|       0|  SF|normal|          tcp|   http|      241|\n",
      "|     1837|       0|  SF|normal|          tcp|   http|      260|\n",
      "|      261|       0|  SF|normal|          tcp|   http|      241|\n",
      "|      818|       0|  SF|normal|          tcp|   http|      257|\n",
      "|      255|       0|  SF|normal|          tcp|   http|      233|\n",
      "|      504|       0|  SF|normal|          tcp|   http|      233|\n",
      "+---------+--------+----+------+-------------+-------+---------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "interactions_labeled_df.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Note: We don't have to register the schema if we do not plan on using SQL queries on the table."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------+------+\n",
      "| label| count|\n",
      "+------+------+\n",
      "|normal| 97278|\n",
      "|attack|396743|\n",
      "+------+------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "(interactions_labeled_df\n",
    " .select(\"label\")\n",
    " .groupBy(\"label\")\n",
    " .count()\n",
    " .show())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------+-------------+------+\n",
      "| label|protocol_type| count|\n",
      "+------+-------------+------+\n",
      "|normal|          udp| 19177|\n",
      "|normal|         icmp|  1288|\n",
      "|normal|          tcp| 76813|\n",
      "|attack|         icmp|282314|\n",
      "|attack|          tcp|113252|\n",
      "|attack|          udp|  1177|\n",
      "+------+-------------+------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Count interactions them by label and protocol type, \n",
    "# in order to see how important the protocol type is to detect when an interaction is or not an attack.\n",
    "\n",
    "(interactions_labeled_df\n",
    " .select(\"label\", \"protocol_type\")\n",
    " .groupBy(\"label\", \"protocol_type\")\n",
    " .count()\n",
    " .show())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------+-------------+---------------+------+\n",
      "| label|protocol_type|(dst_bytes = 0)| count|\n",
      "+------+-------------+---------------+------+\n",
      "|normal|          udp|          false| 15583|\n",
      "|attack|          udp|          false|    11|\n",
      "|attack|          tcp|           true|110583|\n",
      "|normal|          tcp|          false| 67500|\n",
      "|attack|         icmp|           true|282314|\n",
      "|attack|          tcp|          false|  2669|\n",
      "|normal|          tcp|           true|  9313|\n",
      "|normal|          udp|           true|  3594|\n",
      "|normal|         icmp|           true|  1288|\n",
      "|attack|          udp|           true|  1166|\n",
      "+------+-------------+---------------+------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Possible to do more complex groupings by throwing a Bool in the mix\n",
    "\n",
    "(interactions_labeled_df\n",
    " .select(\"label\", \"protocol_type\", \"dst_bytes\")\n",
    " .groupBy(\"label\", \"protocol_type\", interactions_labeled_df.dst_bytes==0)\n",
    " .count()\n",
    " .show())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> ### TASK 1. Count the number of interactions by label and protocol_type using a SQL query"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "interactions_labeled_df.registerTempTable(\"int_lab\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------+-------------+--------+\n",
      "| label|protocol_type|count(1)|\n",
      "+------+-------------+--------+\n",
      "|normal|          udp|   19177|\n",
      "|normal|         icmp|    1288|\n",
      "|normal|          tcp|   76813|\n",
      "|attack|         icmp|  282314|\n",
      "|attack|          tcp|  113252|\n",
      "|attack|          udp|    1177|\n",
      "+------+-------------+--------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "spark.sql(\"\"\"SELECT label, protocol_type, count(*) \n",
    "from int_lab\n",
    "group by label, protocol_type\"\"\").show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Example 2: Flights Data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To add csv import functionality to Spark, launch it with the following option `--packages com.databricks:spark-csv_2.11:1.4.0` such the the launch command becomes:\n",
    "\n",
    "\n",
    "> `IPYTHON_OPTS=\"notebook\" ./bin/pyspark --packages com.databricks:spark-csv_2.11:1.4.0`\n",
    "\n",
    "and then use the `.read()` method of the SQLContext to import the csv file.\n",
    "\n",
    "Syntax:\n",
    "\n",
    "`sparkread.format('com.databricks.spark.csv').options(header='true', inferschema='true').load(<path-to-csv>)`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Year,Month,DayofMonth,DayOfWeek,DepTime,CRSDepTime,ArrTime,CRSArrTime,UniqueCarrier,FlightNum,TailNum,ActualElapsedTime,CRSElapsedTime,AirTime,ArrDelay,DepDelay,Origin,Dest,Distance,TaxiIn,TaxiOut,Cancelled,CancellationCode,Diverted,CarrierDelay,WeatherDelay,NASDelay,SecurityDelay,LateAircraftDelay\r\n",
      "2008,1,3,4,2003,1955,2211,2225,WN,335,N712SW,128,150,116,-14,8,IAD,TPA,810,4,8,0,,0,NA,NA,NA,NA,NA\r\n",
      "2008,1,3,4,754,735,1002,1000,WN,3231,N772SW,128,145,113,2,19,IAD,TPA,810,5,10,0,,0,NA,NA,NA,NA,NA\r\n",
      "2008,1,3,4,628,620,804,750,WN,448,N428WN,96,90,76,14,8,IND,BWI,515,3,17,0,,0,NA,NA,NA,NA,NA\r\n",
      "2008,1,3,4,926,930,1054,1100,WN,1746,N612SW,88,90,78,-6,-4,IND,BWI,515,3,7,0,,0,NA,NA,NA,NA,NA\r\n"
     ]
    }
   ],
   "source": [
    "!head -5 /home/dush/Documents/2008.csv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "7009729 /home/dush/Documents/2008.csv\r\n"
     ]
    }
   ],
   "source": [
    "!wc -l /home/dush/Documents/2008.csv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "\n",
    "spark = SparkSession \\\n",
    "    .builder \\\n",
    "    .appName(\"Python Spark SQL basic example\") \\\n",
    "    .config(\"spark.some.config.option\", \"some-value\") \\\n",
    "    .getOrCreate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "flights = (spark\n",
    "           .read\n",
    "           .format('com.databricks.spark.csv')\n",
    "           .options(header='true', inferschema='true')\n",
    "           .load('/home/dush/Documents/2008.csv'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "pyspark.sql.dataframe.DataFrame"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "type(flights)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('Year', 'int'),\n",
       " ('Month', 'int'),\n",
       " ('DayofMonth', 'int'),\n",
       " ('DayOfWeek', 'int')]"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "flights.dtypes[:4]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "6"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "flights.rdd.getNumPartitions()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "7009728"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "flights.count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- Year: integer (nullable = true)\n",
      " |-- Month: integer (nullable = true)\n",
      " |-- DayofMonth: integer (nullable = true)\n",
      " |-- DayOfWeek: integer (nullable = true)\n",
      " |-- DepTime: string (nullable = true)\n",
      " |-- CRSDepTime: integer (nullable = true)\n",
      " |-- ArrTime: string (nullable = true)\n",
      " |-- CRSArrTime: integer (nullable = true)\n",
      " |-- UniqueCarrier: string (nullable = true)\n",
      " |-- FlightNum: integer (nullable = true)\n",
      " |-- TailNum: string (nullable = true)\n",
      " |-- ActualElapsedTime: string (nullable = true)\n",
      " |-- CRSElapsedTime: string (nullable = true)\n",
      " |-- AirTime: string (nullable = true)\n",
      " |-- ArrDelay: string (nullable = true)\n",
      " |-- DepDelay: string (nullable = true)\n",
      " |-- Origin: string (nullable = true)\n",
      " |-- Dest: string (nullable = true)\n",
      " |-- Distance: integer (nullable = true)\n",
      " |-- TaxiIn: string (nullable = true)\n",
      " |-- TaxiOut: string (nullable = true)\n",
      " |-- Cancelled: integer (nullable = true)\n",
      " |-- CancellationCode: string (nullable = true)\n",
      " |-- Diverted: integer (nullable = true)\n",
      " |-- CarrierDelay: string (nullable = true)\n",
      " |-- WeatherDelay: string (nullable = true)\n",
      " |-- NASDelay: string (nullable = true)\n",
      " |-- SecurityDelay: string (nullable = true)\n",
      " |-- LateAircraftDelay: string (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "flights.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------------+\n",
      "|    avg(Distance)|\n",
      "+-----------------+\n",
      "|726.3870294253928|\n",
      "+-----------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "flights.agg({'Distance':'avg'}).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## Machine Learning in Spark - $teaser$\n",
    "\n",
    "## MLlib\n",
    "Apache Spark provides a general machine learning library — MLlib — that is designed for simplicity, scalability, and easy integration with other tools. With the scalability, language compatibility, and speed of Spark, data scientists can solve and iterate through their data problems faster.\n",
    "\n",
    "From the inception of the Apache Spark project, MLlib was considered foundational for Spark’s success. The key benefit of MLlib is that it allows data scientists to focus on their data problems and models instead of solving the complexities surrounding distributed data (such as infrastructure, configurations, and so on). The data engineers can focus on distributed systems engineering using Spark’s easy-to-use APIs, while the data scientists can leverage the scale and speed of Spark core. Just as important, Spark MLlib is a general-purpose library, providing algorithms for most use cases while at the same time allowing the community to build upon and extend it for specialized use cases. To review the key terms of machine learning, please refer to Matthew Mayo’s Machine Learning Key Terms, Explained.\n",
    "\n",
    "## ML Pipelines\n",
    "Typically when running machine learning algorithms, it involves a sequence of tasks including pre-processing, feature extraction, model fitting, and validation stages. For example, when classifying text documents might involve text segmentation and cleaning, extracting features, and training a classification model with cross-validation. Though there are many libraries we can use for each stage, connecting the dots is not as easy as it may look, especially with large-scale datasets. Most ML libraries are not designed for distributed computation or they do not provide native support for pipeline creation and tuning.\n",
    "\n",
    "<img src=\"https://databricks.com/wp-content/uploads/2016/06/ML-pipelines-diagram.png\">\n",
    "\n",
    "The ML Pipelines is a High-Level API for MLlib that lives under the “spark.ml” package. A pipeline consists of a sequence of stages. There are two basic types of pipeline stages: Transformer and Estimator. A Transformer takes a dataset as input and produces an augmented dataset as output. E.g., a tokenizer is a Transformer that transforms a dataset with text into an dataset with tokenized words. An Estimator must be first fit on the input dataset to produce a model, which is a Transformer that transforms the input dataset. E.g., logistic regression is an Estimator that trains on a dataset with labels and features and produces a logistic regression model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
